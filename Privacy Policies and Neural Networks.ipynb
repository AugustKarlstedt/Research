{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network Classification of Privacy Policy Data Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## August Karlstedt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import imp\n",
    "import operator\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "#import pickle\n",
    "#from six.moves import urllib\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import metrics\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "#import fasttext\n",
    "# https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "\n",
    "import gensim\n",
    "# https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup the categories that we'll try to classify. We'll also need one-hot encodings that for the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the OPP paper https://www.usableprivacy.org/static/files/swilson_acl_2016.pdf:\n",
    "\n",
    "1. **First Party Collection/Use**: how and why a service provider collects user information.\n",
    "2. **Third Party Sharing/Collection**: how user information may be shared with or collected by third parties. \n",
    "3. **User Choice/Control**: choices and control options available to users. \n",
    "4. **User Access, Edit, & Deletion**: if and how users may access, edit, or delete their information. \n",
    "5. **Data Retention**: how long user information is stored. \n",
    "6. **Data Security**: how user information is protected. \n",
    "7. **Policy Change**: if and how users will be in formed about changes to the privacy policy. \n",
    "8. **Do Not Track**: if and how Do Not Track signals 3 for online tracking and advertising are honored. \n",
    "9. **International & Specific Audiences**: practices that pertain only to a specific group of users (e.g., children, Europeans, or California residents). \n",
    "10. **Other**: additional sublabels for introductory or general text, contact information, and practices not covered by the other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \n",
    "    files = []\n",
    "    data = []\n",
    "    header = ['Annotation ID', 'Batch ID', 'Annotator ID', 'Policy ID', 'Segment ID', 'Category Name', 'Attributes/Values', 'Policy URL', 'Date']\n",
    "    keep_columns = ['Segment ID', 'Category Name', 'Attributes/Values']\n",
    "    for file in glob.glob(\"data\\\\annotations/*.csv\"):\n",
    "        files.append(file[17:-4])\n",
    "        data.append(pd.read_csv(file, names=header)[keep_columns])\n",
    "        \n",
    "    policies = []\n",
    "    for file in files:\n",
    "        with open(\"data\\\\sanitized_policies/{}.html\".format(file)) as f:\n",
    "            policies.append(f.readlines()[0].split('|||'))\n",
    "\n",
    "    # categories = set()\n",
    "    # for datum in data:\n",
    "    #     cat = datum['Category Name']\n",
    "    #     categories.update(cat)\n",
    "    # categories\n",
    "\n",
    "    categories = [\n",
    "     'Data Retention', # 0\n",
    "     'Data Security', # 1\n",
    "     'Do Not Track', # 2\n",
    "     'First Party Collection/Use', # 3\n",
    "     'International and Specific Audiences', # 4\n",
    "     'Other', # 5\n",
    "     'Policy Change', # 6\n",
    "     'Third Party Sharing/Collection', # 7\n",
    "     'User Access, Edit and Deletion', # 8\n",
    "     'User Choice/Control', # 9\n",
    "     'None' # 10\n",
    "    ]\n",
    "\n",
    "    one_hot_categories = np.identity(len(categories))\n",
    "\n",
    "    cat_dict = {\n",
    "     categories[0]:  one_hot_categories[0],\n",
    "     categories[1]:  one_hot_categories[1],\n",
    "     categories[2]:  one_hot_categories[2],\n",
    "     categories[3]:  one_hot_categories[3],\n",
    "     categories[4]:  one_hot_categories[4],\n",
    "     categories[5]:  one_hot_categories[5],\n",
    "     categories[6]:  one_hot_categories[6],\n",
    "     categories[7]:  one_hot_categories[7],\n",
    "     categories[8]:  one_hot_categories[8],\n",
    "     categories[9]:  one_hot_categories[9],\n",
    "     categories[10]: one_hot_categories[10],\n",
    "    }\n",
    "\n",
    "    # attribute_value_types = set()\n",
    "    # attribute_value_values = set()\n",
    "    # for datum in data:\n",
    "    #     avs = datum['Attributes/Values']\n",
    "    #     for row in avs:\n",
    "    #         parsed = json.loads(row)\n",
    "    #         keys = list(parsed.keys())\n",
    "    #         attribute_value_types.update(keys)\n",
    "    #         for key in keys:\n",
    "    #             attribute_value_values.add(parsed[key]['value'])\n",
    "\n",
    "    attribute_value_types = ['Access Scope',\n",
    "     'Access Type',\n",
    "     'Action First-Party',\n",
    "     'Action Third Party',\n",
    "     'Audience Type',\n",
    "     'Change Type',\n",
    "     'Choice Scope',\n",
    "     'Choice Type',\n",
    "     'Collection Mode',\n",
    "     'Do Not Track policy',\n",
    "     'Does/Does Not',\n",
    "     'Identifiability',\n",
    "     'Notification Type',\n",
    "     'Other Type',\n",
    "     'Personal Information Type',\n",
    "     'Purpose',\n",
    "     'Retention Period',\n",
    "     'Retention Purpose',\n",
    "     'Security Measure',\n",
    "     'Third Party Entity',\n",
    "     'User Choice',\n",
    "     'User Type']\n",
    "\n",
    "    attribute_value_values = ['Additional service/feature',\n",
    "     'Advertising',\n",
    "     'Aggregated or anonymized',\n",
    "     'Analytics/Research',\n",
    "     'Basic service/feature',\n",
    "     'Both',\n",
    "     'Browser/device privacy controls',\n",
    "     'Californians',\n",
    "     'Children',\n",
    "     'Citizens from other countries',\n",
    "     'Collect from user on other websites',\n",
    "     'Collect in mobile app',\n",
    "     'Collect on first party website/app',\n",
    "     'Collect on mobile website',\n",
    "     'Collect on website',\n",
    "     'Collection',\n",
    "     'Computer information',\n",
    "     'Contact',\n",
    "     'Cookies and tracking elements',\n",
    "     'Data access limitation',\n",
    "     'Deactivate account',\n",
    "     'Delete account (full)',\n",
    "     'Delete account (partial)',\n",
    "     'Demographic',\n",
    "     'Does',\n",
    "     'Does Not',\n",
    "     'Dont use service/feature',\n",
    "     'Edit information',\n",
    "     'Europeans',\n",
    "     'Explicit',\n",
    "     'Export',\n",
    "     'Financial',\n",
    "     'First party collection',\n",
    "     'First party use',\n",
    "     'First-party privacy controls',\n",
    "     'General notice in privacy policy',\n",
    "     'General notice on website',\n",
    "     'Generic',\n",
    "     'Generic personal information',\n",
    "     'Health',\n",
    "     'Honored',\n",
    "     'IP address and device IDs',\n",
    "     'Identifiable',\n",
    "     'Implicit',\n",
    "     'In case of merger or acquisition',\n",
    "     'Indefinitely',\n",
    "     'Introductory/Generic',\n",
    "     'Legal requirement',\n",
    "     'Limited',\n",
    "     'Location',\n",
    "     'Marketing',\n",
    "     'Mentioned, but unclear if honored',\n",
    "     'Merger/Acquisition',\n",
    "     'Named third party',\n",
    "     'No notification',\n",
    "     'Non-privacy relevant change',\n",
    "     'None',\n",
    "     'Not honored',\n",
    "     'Not mentioned',\n",
    "     'Opt-in',\n",
    "     'Opt-out',\n",
    "     'Opt-out link',\n",
    "     'Opt-out via contacting company',\n",
    "     'Other',\n",
    "     'Other data about user',\n",
    "     'Other part of company/affiliate',\n",
    "     'Other users',\n",
    "     'Perform service',\n",
    "     'Personal identifier',\n",
    "     'Personal notice',\n",
    "     'Personalization/Customization',\n",
    "     'Practice not covered',\n",
    "     'Privacy contact information',\n",
    "     'Privacy relevant change',\n",
    "     'Privacy review/audit',\n",
    "     'Privacy training',\n",
    "     'Privacy/Security program',\n",
    "     'Profile data',\n",
    "     'Public',\n",
    "     'Receive from other parts of company/affiliates',\n",
    "     'Receive from other service/third-party (named)',\n",
    "     'Receive from other service/third-party (unnamed)',\n",
    "     'Receive/Shared with',\n",
    "     'Secure data storage',\n",
    "     'Secure data transfer',\n",
    "     'Secure user authentication',\n",
    "     'See',\n",
    "     'Service Operation and Security',\n",
    "     'Service operation and security',\n",
    "     'Social media data',\n",
    "     'Stated Period',\n",
    "     'Survey data',\n",
    "     'Third party sharing/collection',\n",
    "     'Third party use',\n",
    "     'Third-party privacy controls',\n",
    "     'Track on first party website/app',\n",
    "     'Track user on other websites',\n",
    "     'Transactional data',\n",
    "     'Unnamed third party',\n",
    "     'Unspecified',\n",
    "     'Use',\n",
    "     'User Profile',\n",
    "     'User account data',\n",
    "     'User online activities',\n",
    "     'User participation',\n",
    "     'User profile',\n",
    "     'User with account',\n",
    "     'User without account',\n",
    "     'View',\n",
    "     'not-selected']\n",
    "\n",
    "    model = gensim.models.Doc2Vec(size=100)\n",
    "\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    chosen_categories = ['First Party Collection/Use', \n",
    "                         'Third Party Sharing/Collection', \n",
    "                         'Other', \n",
    "                         'User Choice/Control', \n",
    "                         'Data Security',\n",
    "                         'International and Specific Audiences',\n",
    "                         'User Access, Edit and Deletion',\n",
    "                         'Policy Change',\n",
    "                         'Data Retention',\n",
    "                         'Do Not Track',\n",
    "                         'None' # added by us, not in original corpus\n",
    "                        ]\n",
    "    remove_text = ['null', 'Not selected']\n",
    "\n",
    "    df_columns = ['text', 'category', 'category one hot', 'text vec']\n",
    "    df = pd.DataFrame([], columns=df_columns)\n",
    "    series = []\n",
    "    documents = []\n",
    "    cats = []\n",
    "\n",
    "    remove_spans = {} # dictionary of policy ids and list of start, stop tuples that are then removed\n",
    "    # remove_spans structure:\n",
    "\n",
    "    '''\n",
    "    {\n",
    "    \"2\": --> this is the policy id\n",
    "      {\n",
    "       \"6\": [(20, 30), (30, 50)], --> this is the segment id\n",
    "       \"8\": [(40, 123)] --> which maps to a list of tuple of start, end indices\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "\n",
    "\n",
    "    idx = 0\n",
    "    for datum_idx in range(len(data)):\n",
    "        datum = data[datum_idx]\n",
    "        for idx in range(len(datum)):        \n",
    "            category = datum['Category Name'][idx]\n",
    "\n",
    "            if chosen_categories is None:\n",
    "                continue\n",
    "\n",
    "            if category not in chosen_categories:\n",
    "                continue\n",
    "\n",
    "            segment_id = datum['Segment ID'][idx]\n",
    "            if datum_idx not in remove_spans:\n",
    "                remove_spans[datum_idx] = {}\n",
    "            if segment_id not in remove_spans[datum_idx]:\n",
    "                remove_spans[datum_idx][segment_id] = []\n",
    "\n",
    "            # ok, we have our policy text, now we need to \n",
    "            # remove all of the spans that are associated with a category\n",
    "            # so we can attribute that text to the 'None' category\n",
    "\n",
    "            parsed = json.loads(datum['Attributes/Values'][idx])\n",
    "            for value in attribute_value_types:\n",
    "                if value in parsed.keys():\n",
    "                    attributes = parsed[value]\n",
    "                    has_selected_text = 'selectedText' in attributes\n",
    "                    has_start_idx = 'startIndexInSegment' in attributes\n",
    "                    has_end_idx = 'endIndexInSegment' in attributes\n",
    "                    if has_selected_text and has_start_idx and has_end_idx:\n",
    "                        text = attributes['selectedText']\n",
    "                        start_idx = attributes['startIndexInSegment']\n",
    "                        end_idx = attributes['endIndexInSegment']\n",
    "\n",
    "                        if text in remove_text or start_idx == -1 or end_idx == -1:\n",
    "                            continue\n",
    "\n",
    "                        remove_spans[datum_idx][segment_id].append((start_idx, end_idx))\n",
    "\n",
    "                        text = text.lower()\n",
    "                        processed_text = word_tokenize(text)\n",
    "                        #processed_text = [stemmer.stem(word) for word in processed_text]\n",
    "                        processed_text = [lemmatizer.lemmatize(word) for word in processed_text]\n",
    "\n",
    "                        doc = gensim.models.doc2vec.TaggedDocument(processed_text, [idx])\n",
    "                        documents.append(doc)\n",
    "                        cats.append(cat_dict[category])\n",
    "                        text = ' '.join(processed_text)\n",
    "                        series.append(pd.Series([text, category, cat_dict[category], None], index=df_columns))\n",
    "\n",
    "                        idx += 1\n",
    "\n",
    "    SHOULD_PROCESS_NONE_CATEGORY = True\n",
    "\n",
    "    replace_items = [\"<br>\", \"<strong>\", \"</strong>\", \"<ul>\", \"</ul>\", \"<li>\", \"</li>\", \"<ol>\", \"</ol>\"]\n",
    "    category = 'None'\n",
    "    none_count = 0\n",
    "\n",
    "    if SHOULD_PROCESS_NONE_CATEGORY:\n",
    "        for policy_idx in remove_spans:\n",
    "            policy = policies[policy_idx]\n",
    "            for segment_idx in remove_spans[policy_idx]:\n",
    "                try:\n",
    "                    policy_segment = policy[segment_idx]\n",
    "                except IndexError as e:\n",
    "                    print(e, policy_idx, segment_idx)\n",
    "                    continue\n",
    "                segment_text = policy_segment\n",
    "                for span in remove_spans[policy_idx][segment_idx]:\n",
    "                    start_idx = span[0]\n",
    "                    end_idx = span[1]\n",
    "                    segment_text = segment_text[:start_idx] + \" \" + segment_text[end_idx:]\n",
    "                segment_text = segment_text.lower()\n",
    "                for item in replace_items:\n",
    "                    segment_text = segment_text.replace(item, \" \")\n",
    "                segment_text = segment_text.strip()\n",
    "                if segment_text: # check if we have any characters at all\n",
    "                    processed_text = word_tokenize(segment_text)\n",
    "                    processed_text = [lemmatizer.lemmatize(word) for word in processed_text]\n",
    "\n",
    "                    doc = gensim.models.doc2vec.TaggedDocument(processed_text, [idx])\n",
    "                    documents.append(doc)\n",
    "                    cats.append(cat_dict[category])\n",
    "                    text = ' '.join(processed_text)\n",
    "                    series.append(pd.Series([text, category, cat_dict[category], None], index=df_columns))\n",
    "                    none_count += 1\n",
    "                    idx += 1\n",
    "\n",
    "        print('None count: {}'.format(none_count))\n",
    "\n",
    "        cats = np.array(cats)\n",
    "\n",
    "        df = df.append(series, ignore_index=True)\n",
    "        print(df.shape)\n",
    "\n",
    "        model.build_vocab(documents)\n",
    "        model.train(documents, total_examples=len(documents), epochs=16)\n",
    "\n",
    "        vecs = []\n",
    "        for row in df.itertuples():\n",
    "            category_not_chosen = chosen_categories is None\n",
    "            category_chosen_and_matches = chosen_categories is not None and row.category in chosen_categories\n",
    "            if category_chosen_and_matches or category_not_chosen:\n",
    "                model.random = np.random.RandomState(1234)\n",
    "                vecs.append(np.array(model.infer_vector(word_tokenize(row.text))))\n",
    "\n",
    "        vecs = np.array(vecs)\n",
    "        print(vecs.shape)\n",
    "\n",
    "        return vecs, cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Since these Doc2Vec vectors are our only input data right now, let's just use them directly as our input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \n",
    "    vecs, cats = get_data()\n",
    "    \n",
    "    choice = np.random.choice(len(vecs), len(vecs), replace=False)\n",
    "    test_percentage = 0.25 # keep 25% of data for testing\n",
    "    test_amount = math.floor(0.25 * len(vecs))\n",
    "    train_indices = choice[test_amount:]\n",
    "    test_indices = choice[:test_amount]\n",
    "    \n",
    "    # vecs, cats\n",
    "    x_train = vecs[train_indices]\n",
    "    x_test = vecs[test_indices]\n",
    "    y_train = cats[train_indices]\n",
    "    y_test = cats[test_indices]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll use a simple neural network consisting of:\n",
    "\n",
    "1. **Fully connected** layer with 256 nodes, relu activation\n",
    "2. **Dropout** 25% of the inputs\n",
    "3. **Fully connected** layer with 256 nodes, relu activation\n",
    "4. **Dropout** 25% of the inputs\n",
    "5. **Fully connected** layer with 11 nodes, softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Dense(256, batch_input_shape=(None, 100, )))\n",
    "    nn_model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    nn_model.add(Dropout({{uniform(0, 1)}}))\n",
    "    nn_model.add(Dense({{choice([256, 512, 1024])}}))\n",
    "    nn_model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    nn_model.add(Dropout({{uniform(0, 1)}}))\n",
    "    nn_model.add(Dense(11))\n",
    "    nn_model.add(Activation('softmax'))\n",
    "\n",
    "    nn_model.compile(loss='categorical_crossentropy', optimizer={{choice(['rmsprop', 'adam', 'sgd'])}}, metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    print(nn_model.summary())\n",
    "\n",
    "    tensorboard_callback = TensorBoard(log_dir='C:/tmp/pp_run-'+time.strftime(\"%Y-%m-%d-%H%M%S\"))\n",
    "    nn_model.fit(x_train, y_train, batch_size={{choice([64, 128])}}, epochs=1, verbose=2, validation_data=(x_test, y_test), callbacks=[tensorboard_callback])\n",
    "    score, acc = nn_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': nn_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have TensorBoard running, you'll see it in the IFrame below!\n",
    "\n",
    "(With default settings, pointing to http://localhost:6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10f9efa9518>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame('http://localhost:6006', '100%', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import imp\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import operator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import math\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import glob\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from IPython.display import IFrame\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import nltk\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nltk.tokenize import word_tokenize\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sn\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Dense, Activation, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM, TimeDistributed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import TensorBoard\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import metrics\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils.np_utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import gensim\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense': hp.choice('Dense', [256, 512, 1024]),\n",
      "        'Activation_1': hp.choice('Activation_1', ['relu', 'sigmoid']),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
      "        'batch_size': hp.choice('batch_size', [64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Functions\n",
      "    1: def get_data():\n",
      "    2:     \n",
      "    3:     files = []\n",
      "    4:     data = []\n",
      "    5:     header = ['Annotation ID', 'Batch ID', 'Annotator ID', 'Policy ID', 'Segment ID', 'Category Name', 'Attributes/Values', 'Policy URL', 'Date']\n",
      "    6:     keep_columns = ['Segment ID', 'Category Name', 'Attributes/Values']\n",
      "    7:     for file in glob.glob(\"data\\\\annotations/*.csv\"):\n",
      "    8:         files.append(file[17:-4])\n",
      "    9:         data.append(pd.read_csv(file, names=header)[keep_columns])\n",
      "   10:         \n",
      "   11:     policies = []\n",
      "   12:     for file in files:\n",
      "   13:         with open(\"data\\\\sanitized_policies/{}.html\".format(file)) as f:\n",
      "   14:             policies.append(f.readlines()[0].split('|||'))\n",
      "   15: \n",
      "   16:     # categories = set()\n",
      "   17:     # for datum in data:\n",
      "   18:     #     cat = datum['Category Name']\n",
      "   19:     #     categories.update(cat)\n",
      "   20:     # categories\n",
      "   21: \n",
      "   22:     categories = [\n",
      "   23:      'Data Retention', # 0\n",
      "   24:      'Data Security', # 1\n",
      "   25:      'Do Not Track', # 2\n",
      "   26:      'First Party Collection/Use', # 3\n",
      "   27:      'International and Specific Audiences', # 4\n",
      "   28:      'Other', # 5\n",
      "   29:      'Policy Change', # 6\n",
      "   30:      'Third Party Sharing/Collection', # 7\n",
      "   31:      'User Access, Edit and Deletion', # 8\n",
      "   32:      'User Choice/Control', # 9\n",
      "   33:      'None' # 10\n",
      "   34:     ]\n",
      "   35: \n",
      "   36:     one_hot_categories = np.identity(len(categories))\n",
      "   37: \n",
      "   38:     cat_dict = {\n",
      "   39:      categories[0]:  one_hot_categories[0],\n",
      "   40:      categories[1]:  one_hot_categories[1],\n",
      "   41:      categories[2]:  one_hot_categories[2],\n",
      "   42:      categories[3]:  one_hot_categories[3],\n",
      "   43:      categories[4]:  one_hot_categories[4],\n",
      "   44:      categories[5]:  one_hot_categories[5],\n",
      "   45:      categories[6]:  one_hot_categories[6],\n",
      "   46:      categories[7]:  one_hot_categories[7],\n",
      "   47:      categories[8]:  one_hot_categories[8],\n",
      "   48:      categories[9]:  one_hot_categories[9],\n",
      "   49:      categories[10]: one_hot_categories[10],\n",
      "   50:     }\n",
      "   51: \n",
      "   52:     # attribute_value_types = set()\n",
      "   53:     # attribute_value_values = set()\n",
      "   54:     # for datum in data:\n",
      "   55:     #     avs = datum['Attributes/Values']\n",
      "   56:     #     for row in avs:\n",
      "   57:     #         parsed = json.loads(row)\n",
      "   58:     #         keys = list(parsed.keys())\n",
      "   59:     #         attribute_value_types.update(keys)\n",
      "   60:     #         for key in keys:\n",
      "   61:     #             attribute_value_values.add(parsed[key]['value'])\n",
      "   62: \n",
      "   63:     attribute_value_types = ['Access Scope',\n",
      "   64:      'Access Type',\n",
      "   65:      'Action First-Party',\n",
      "   66:      'Action Third Party',\n",
      "   67:      'Audience Type',\n",
      "   68:      'Change Type',\n",
      "   69:      'Choice Scope',\n",
      "   70:      'Choice Type',\n",
      "   71:      'Collection Mode',\n",
      "   72:      'Do Not Track policy',\n",
      "   73:      'Does/Does Not',\n",
      "   74:      'Identifiability',\n",
      "   75:      'Notification Type',\n",
      "   76:      'Other Type',\n",
      "   77:      'Personal Information Type',\n",
      "   78:      'Purpose',\n",
      "   79:      'Retention Period',\n",
      "   80:      'Retention Purpose',\n",
      "   81:      'Security Measure',\n",
      "   82:      'Third Party Entity',\n",
      "   83:      'User Choice',\n",
      "   84:      'User Type']\n",
      "   85: \n",
      "   86:     attribute_value_values = ['Additional service/feature',\n",
      "   87:      'Advertising',\n",
      "   88:      'Aggregated or anonymized',\n",
      "   89:      'Analytics/Research',\n",
      "   90:      'Basic service/feature',\n",
      "   91:      'Both',\n",
      "   92:      'Browser/device privacy controls',\n",
      "   93:      'Californians',\n",
      "   94:      'Children',\n",
      "   95:      'Citizens from other countries',\n",
      "   96:      'Collect from user on other websites',\n",
      "   97:      'Collect in mobile app',\n",
      "   98:      'Collect on first party website/app',\n",
      "   99:      'Collect on mobile website',\n",
      "  100:      'Collect on website',\n",
      "  101:      'Collection',\n",
      "  102:      'Computer information',\n",
      "  103:      'Contact',\n",
      "  104:      'Cookies and tracking elements',\n",
      "  105:      'Data access limitation',\n",
      "  106:      'Deactivate account',\n",
      "  107:      'Delete account (full)',\n",
      "  108:      'Delete account (partial)',\n",
      "  109:      'Demographic',\n",
      "  110:      'Does',\n",
      "  111:      'Does Not',\n",
      "  112:      'Dont use service/feature',\n",
      "  113:      'Edit information',\n",
      "  114:      'Europeans',\n",
      "  115:      'Explicit',\n",
      "  116:      'Export',\n",
      "  117:      'Financial',\n",
      "  118:      'First party collection',\n",
      "  119:      'First party use',\n",
      "  120:      'First-party privacy controls',\n",
      "  121:      'General notice in privacy policy',\n",
      "  122:      'General notice on website',\n",
      "  123:      'Generic',\n",
      "  124:      'Generic personal information',\n",
      "  125:      'Health',\n",
      "  126:      'Honored',\n",
      "  127:      'IP address and device IDs',\n",
      "  128:      'Identifiable',\n",
      "  129:      'Implicit',\n",
      "  130:      'In case of merger or acquisition',\n",
      "  131:      'Indefinitely',\n",
      "  132:      'Introductory/Generic',\n",
      "  133:      'Legal requirement',\n",
      "  134:      'Limited',\n",
      "  135:      'Location',\n",
      "  136:      'Marketing',\n",
      "  137:      'Mentioned, but unclear if honored',\n",
      "  138:      'Merger/Acquisition',\n",
      "  139:      'Named third party',\n",
      "  140:      'No notification',\n",
      "  141:      'Non-privacy relevant change',\n",
      "  142:      'None',\n",
      "  143:      'Not honored',\n",
      "  144:      'Not mentioned',\n",
      "  145:      'Opt-in',\n",
      "  146:      'Opt-out',\n",
      "  147:      'Opt-out link',\n",
      "  148:      'Opt-out via contacting company',\n",
      "  149:      'Other',\n",
      "  150:      'Other data about user',\n",
      "  151:      'Other part of company/affiliate',\n",
      "  152:      'Other users',\n",
      "  153:      'Perform service',\n",
      "  154:      'Personal identifier',\n",
      "  155:      'Personal notice',\n",
      "  156:      'Personalization/Customization',\n",
      "  157:      'Practice not covered',\n",
      "  158:      'Privacy contact information',\n",
      "  159:      'Privacy relevant change',\n",
      "  160:      'Privacy review/audit',\n",
      "  161:      'Privacy training',\n",
      "  162:      'Privacy/Security program',\n",
      "  163:      'Profile data',\n",
      "  164:      'Public',\n",
      "  165:      'Receive from other parts of company/affiliates',\n",
      "  166:      'Receive from other service/third-party (named)',\n",
      "  167:      'Receive from other service/third-party (unnamed)',\n",
      "  168:      'Receive/Shared with',\n",
      "  169:      'Secure data storage',\n",
      "  170:      'Secure data transfer',\n",
      "  171:      'Secure user authentication',\n",
      "  172:      'See',\n",
      "  173:      'Service Operation and Security',\n",
      "  174:      'Service operation and security',\n",
      "  175:      'Social media data',\n",
      "  176:      'Stated Period',\n",
      "  177:      'Survey data',\n",
      "  178:      'Third party sharing/collection',\n",
      "  179:      'Third party use',\n",
      "  180:      'Third-party privacy controls',\n",
      "  181:      'Track on first party website/app',\n",
      "  182:      'Track user on other websites',\n",
      "  183:      'Transactional data',\n",
      "  184:      'Unnamed third party',\n",
      "  185:      'Unspecified',\n",
      "  186:      'Use',\n",
      "  187:      'User Profile',\n",
      "  188:      'User account data',\n",
      "  189:      'User online activities',\n",
      "  190:      'User participation',\n",
      "  191:      'User profile',\n",
      "  192:      'User with account',\n",
      "  193:      'User without account',\n",
      "  194:      'View',\n",
      "  195:      'not-selected']\n",
      "  196: \n",
      "  197:     model = gensim.models.Doc2Vec(size=100)\n",
      "  198: \n",
      "  199:     stemmer = nltk.stem.porter.PorterStemmer()\n",
      "  200:     lemmatizer = nltk.stem.WordNetLemmatizer()\n",
      "  201: \n",
      "  202:     chosen_categories = ['First Party Collection/Use', \n",
      "  203:                          'Third Party Sharing/Collection', \n",
      "  204:                          'Other', \n",
      "  205:                          'User Choice/Control', \n",
      "  206:                          'Data Security',\n",
      "  207:                          'International and Specific Audiences',\n",
      "  208:                          'User Access, Edit and Deletion',\n",
      "  209:                          'Policy Change',\n",
      "  210:                          'Data Retention',\n",
      "  211:                          'Do Not Track',\n",
      "  212:                          'None' # added by us, not in original corpus\n",
      "  213:                         ]\n",
      "  214:     remove_text = ['null', 'Not selected']\n",
      "  215: \n",
      "  216:     df_columns = ['text', 'category', 'category one hot', 'text vec']\n",
      "  217:     df = pd.DataFrame([], columns=df_columns)\n",
      "  218:     series = []\n",
      "  219:     documents = []\n",
      "  220:     cats = []\n",
      "  221: \n",
      "  222:     remove_spans = {} # dictionary of policy ids and list of start, stop tuples that are then removed\n",
      "  223:     # remove_spans structure:\n",
      "  224: \n",
      "  225:     '''\n",
      "  226:     {\n",
      "  227:     \"2\": --> this is the policy id\n",
      "  228:       {\n",
      "  229:        \"6\": [(20, 30), (30, 50)], --> this is the segment id\n",
      "  230:        \"8\": [(40, 123)] --> which maps to a list of tuple of start, end indices\n",
      "  231:       }\n",
      "  232:     }\n",
      "  233:     '''\n",
      "  234: \n",
      "  235: \n",
      "  236:     idx = 0\n",
      "  237:     for datum_idx in range(len(data)):\n",
      "  238:         datum = data[datum_idx]\n",
      "  239:         for idx in range(len(datum)):        \n",
      "  240:             category = datum['Category Name'][idx]\n",
      "  241: \n",
      "  242:             if chosen_categories is None:\n",
      "  243:                 continue\n",
      "  244: \n",
      "  245:             if category not in chosen_categories:\n",
      "  246:                 continue\n",
      "  247: \n",
      "  248:             segment_id = datum['Segment ID'][idx]\n",
      "  249:             if datum_idx not in remove_spans:\n",
      "  250:                 remove_spans[datum_idx] = {}\n",
      "  251:             if segment_id not in remove_spans[datum_idx]:\n",
      "  252:                 remove_spans[datum_idx][segment_id] = []\n",
      "  253: \n",
      "  254:             # ok, we have our policy text, now we need to \n",
      "  255:             # remove all of the spans that are associated with a category\n",
      "  256:             # so we can attribute that text to the 'None' category\n",
      "  257: \n",
      "  258:             parsed = json.loads(datum['Attributes/Values'][idx])\n",
      "  259:             for value in attribute_value_types:\n",
      "  260:                 if value in parsed.keys():\n",
      "  261:                     attributes = parsed[value]\n",
      "  262:                     has_selected_text = 'selectedText' in attributes\n",
      "  263:                     has_start_idx = 'startIndexInSegment' in attributes\n",
      "  264:                     has_end_idx = 'endIndexInSegment' in attributes\n",
      "  265:                     if has_selected_text and has_start_idx and has_end_idx:\n",
      "  266:                         text = attributes['selectedText']\n",
      "  267:                         start_idx = attributes['startIndexInSegment']\n",
      "  268:                         end_idx = attributes['endIndexInSegment']\n",
      "  269: \n",
      "  270:                         if text in remove_text or start_idx == -1 or end_idx == -1:\n",
      "  271:                             continue\n",
      "  272: \n",
      "  273:                         remove_spans[datum_idx][segment_id].append((start_idx, end_idx))\n",
      "  274: \n",
      "  275:                         text = text.lower()\n",
      "  276:                         processed_text = word_tokenize(text)\n",
      "  277:                         #processed_text = [stemmer.stem(word) for word in processed_text]\n",
      "  278:                         processed_text = [lemmatizer.lemmatize(word) for word in processed_text]\n",
      "  279: \n",
      "  280:                         doc = gensim.models.doc2vec.TaggedDocument(processed_text, [idx])\n",
      "  281:                         documents.append(doc)\n",
      "  282:                         cats.append(cat_dict[category])\n",
      "  283:                         text = ' '.join(processed_text)\n",
      "  284:                         series.append(pd.Series([text, category, cat_dict[category], None], index=df_columns))\n",
      "  285: \n",
      "  286:                         idx += 1\n",
      "  287: \n",
      "  288:     SHOULD_PROCESS_NONE_CATEGORY = True\n",
      "  289: \n",
      "  290:     replace_items = [\"<br>\", \"<strong>\", \"</strong>\", \"<ul>\", \"</ul>\", \"<li>\", \"</li>\", \"<ol>\", \"</ol>\"]\n",
      "  291:     category = 'None'\n",
      "  292:     none_count = 0\n",
      "  293: \n",
      "  294:     if SHOULD_PROCESS_NONE_CATEGORY:\n",
      "  295:         for policy_idx in remove_spans:\n",
      "  296:             policy = policies[policy_idx]\n",
      "  297:             for segment_idx in remove_spans[policy_idx]:\n",
      "  298:                 try:\n",
      "  299:                     policy_segment = policy[segment_idx]\n",
      "  300:                 except IndexError as e:\n",
      "  301:                     print(e, policy_idx, segment_idx)\n",
      "  302:                     continue\n",
      "  303:                 segment_text = policy_segment\n",
      "  304:                 for span in remove_spans[policy_idx][segment_idx]:\n",
      "  305:                     start_idx = span[0]\n",
      "  306:                     end_idx = span[1]\n",
      "  307:                     segment_text = segment_text[:start_idx] + \" \" + segment_text[end_idx:]\n",
      "  308:                 segment_text = segment_text.lower()\n",
      "  309:                 for item in replace_items:\n",
      "  310:                     segment_text = segment_text.replace(item, \" \")\n",
      "  311:                 segment_text = segment_text.strip()\n",
      "  312:                 if segment_text: # check if we have any characters at all\n",
      "  313:                     processed_text = word_tokenize(segment_text)\n",
      "  314:                     processed_text = [lemmatizer.lemmatize(word) for word in processed_text]\n",
      "  315: \n",
      "  316:                     doc = gensim.models.doc2vec.TaggedDocument(processed_text, [idx])\n",
      "  317:                     documents.append(doc)\n",
      "  318:                     cats.append(cat_dict[category])\n",
      "  319:                     text = ' '.join(processed_text)\n",
      "  320:                     series.append(pd.Series([text, category, cat_dict[category], None], index=df_columns))\n",
      "  321:                     none_count += 1\n",
      "  322:                     idx += 1\n",
      "  323: \n",
      "  324:         print('None count: {}'.format(none_count))\n",
      "  325: \n",
      "  326:         cats = np.array(cats)\n",
      "  327: \n",
      "  328:         df = df.append(series, ignore_index=True)\n",
      "  329:         print(df.shape)\n",
      "  330: \n",
      "  331:         model.build_vocab(documents)\n",
      "  332:         model.train(documents, total_examples=len(documents), epochs=16)\n",
      "  333: \n",
      "  334:         vecs = []\n",
      "  335:         for row in df.itertuples():\n",
      "  336:             category_not_chosen = chosen_categories is None\n",
      "  337:             category_chosen_and_matches = chosen_categories is not None and row.category in chosen_categories\n",
      "  338:             if category_chosen_and_matches or category_not_chosen:\n",
      "  339:                 model.random = np.random.RandomState(1234)\n",
      "  340:                 vecs.append(np.array(model.infer_vector(word_tokenize(row.text))))\n",
      "  341: \n",
      "  342:         vecs = np.array(vecs)\n",
      "  343:         print(vecs.shape)\n",
      "  344: \n",
      "  345:         return vecs, cats\n",
      "  346: \n",
      "  347: \n",
      ">>> Data\n",
      "  1: \n",
      "  2: \n",
      "  3: vecs, cats = get_data()\n",
      "  4: \n",
      "  5: choice = np.random.choice(len(vecs), len(vecs), replace=False)\n",
      "  6: test_percentage = 0.25 # keep 25% of data for testing\n",
      "  7: test_amount = math.floor(0.25 * len(vecs))\n",
      "  8: train_indices = choice[test_amount:]\n",
      "  9: test_indices = choice[:test_amount]\n",
      " 10: \n",
      " 11: # vecs, cats\n",
      " 12: x_train = vecs[train_indices]\n",
      " 13: x_test = vecs[test_indices]\n",
      " 14: y_train = cats[train_indices]\n",
      " 15: y_test = cats[test_indices]\n",
      " 16: \n",
      " 17: \n",
      " 18: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "  1: def keras_fmin_fnct(space):\n",
      "  2: \n",
      "  3: \n",
      "  4:     nn_model = Sequential()\n",
      "  5:     nn_model.add(Dense(256, batch_input_shape=(None, 100, )))\n",
      "  6:     nn_model.add(Activation(space['Activation']))\n",
      "  7:     nn_model.add(Dropout(space['Dropout']))\n",
      "  8:     nn_model.add(Dense(space['Dense']))\n",
      "  9:     nn_model.add(Activation(space['Activation_1']))\n",
      " 10:     nn_model.add(Dropout(space['Dropout_1']))\n",
      " 11:     nn_model.add(Dense(11))\n",
      " 12:     nn_model.add(Activation('softmax'))\n",
      " 13: \n",
      " 14:     nn_model.compile(loss='categorical_crossentropy', optimizer=space['optimizer'], metrics=[metrics.categorical_accuracy])\n",
      " 15: \n",
      " 16:     print(nn_model.summary())\n",
      " 17: \n",
      " 18:     tensorboard_callback = TensorBoard(log_dir='C:/tmp/pp_run-'+time.strftime(\"%Y-%m-%d-%H%M%S\"))\n",
      " 19:     nn_model.fit(x_train, y_train, batch_size=space['batch_size'], epochs=1, verbose=2, validation_data=(x_test, y_test), callbacks=[tensorboard_callback])\n",
      " 20:     score, acc = nn_model.evaluate(x_test, y_test, verbose=0)\n",
      " 21:     print('Test accuracy:', acc)\n",
      " 22:     return {'loss': -acc, 'status': STATUS_OK, 'model': nn_model}\n",
      " 23: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range 47 4\n",
      "list index out of range 47 5\n",
      "list index out of range 47 6\n",
      "list index out of range 47 7\n",
      "list index out of range 47 8\n",
      "list index out of range 47 9\n",
      "list index out of range 47 10\n",
      "list index out of range 47 11\n",
      "list index out of range 47 12\n",
      "list index out of range 47 13\n",
      "list index out of range 47 14\n",
      "list index out of range 47 15\n",
      "list index out of range 47 16\n",
      "list index out of range 47 17\n",
      "list index out of range 47 18\n",
      "list index out of range 47 19\n",
      "list index out of range 47 20\n",
      "list index out of range 47 21\n",
      "list index out of range 47 22\n",
      "list index out of range 47 23\n",
      "list index out of range 47 24\n",
      "list index out of range 47 25\n",
      "list index out of range 47 26\n",
      "list index out of range 47 27\n",
      "list index out of range 47 28\n",
      "list index out of range 47 29\n",
      "list index out of range 47 30\n",
      "list index out of range 47 31\n",
      "list index out of range 47 32\n",
      "list index out of range 47 33\n",
      "list index out of range 47 34\n",
      "list index out of range 47 35\n",
      "list index out of range 47 36\n",
      "list index out of range 47 37\n",
      "list index out of range 47 38\n",
      "list index out of range 47 39\n",
      "list index out of range 47 40\n",
      "list index out of range 47 41\n",
      "list index out of range 47 42\n",
      "list index out of range 47 43\n",
      "list index out of range 47 44\n",
      "list index out of range 47 45\n",
      "list index out of range 47 46\n",
      "None count: 1764\n",
      "(79670, 4)\n",
      "(79670, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 11)                5643      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 11)                0         \n",
      "=================================================================\n",
      "Total params: 163,083\n",
      "Trainable params: 163,083\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 59753 samples, validate on 19917 samples\n",
      "Epoch 1/1\n",
      " - 4s - loss: 1.6694 - categorical_accuracy: 0.4100 - val_loss: 1.4764 - val_categorical_accuracy: 0.4701\n",
      "Test accuracy: 0.47010091881457056\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 11)                11275     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 11)                0         \n",
      "=================================================================\n",
      "Total params: 300,299\n",
      "Trainable params: 300,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 59753 samples, validate on 19917 samples\n",
      "Epoch 1/1\n",
      " - 10s - loss: 1.6920 - categorical_accuracy: 0.4258 - val_loss: 1.7936 - val_categorical_accuracy: 0.4330\n",
      "Test accuracy: 0.4330471456552142\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 11)                11275     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 11)                0         \n",
      "=================================================================\n",
      "Total params: 300,299\n",
      "Trainable params: 300,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 59753 samples, validate on 19917 samples\n",
      "Epoch 1/1\n",
      " - 12s - loss: 1.6769 - categorical_accuracy: 0.4156 - val_loss: 2.3538 - val_categorical_accuracy: 0.0446\n",
      "Test accuracy: 0.04458502786564242\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 11)                2827      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 11)                0         \n",
      "=================================================================\n",
      "Total params: 94,475\n",
      "Trainable params: 94,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 59753 samples, validate on 19917 samples\n",
      "Epoch 1/1\n",
      " - 3s - loss: 1.4837 - categorical_accuracy: 0.4556 - val_loss: 1.3668 - val_categorical_accuracy: 0.4984\n",
      "Test accuracy: 0.4984184365294787\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-574228e015c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbest_run\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Privacy Policies and Neural Networks'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8d708f37b3c0>\u001b[0m in \u001b[0;36mdata\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mchoice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-8df01b7ea472>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m    282\u001b[0m                         \u001b[0mcats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m                         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m                         \u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m                         \u001b[0midx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m                 \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_ensure_index\u001b[1;34m(index_like, copy)\u001b[0m\n\u001b[0;32m   4210\u001b[0m             \u001b[0mindex_like\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4212\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, data, dtype, copy, name, fastpath, tupleize_cols, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m                         \u001b[1;32mexcept\u001b[0m \u001b[0mIncompatibleFrequency\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__array__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_simple_new\u001b[1;34m(cls, values, name, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset_identity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "functions=[get_data]\n",
    "best_run, best_model = optim.minimize(model=create_model, data=data, functions=functions, algo=tpe.suggest, max_evals=4, trials=Trials(), notebook_name='Privacy Policies and Neural Networks')\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a confusion matrix to see the accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, just to get some stats on each category, the number of items in each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_lengths = {}\n",
    "\n",
    "for category in categories:\n",
    "    category_lengths[category] = len(df.loc[df['category'] == category])\n",
    "    print(category_lengths[category], 'examples in the', category, 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate our confusion matrix, we need the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix_columns = categories\n",
    "\n",
    "confusion_matrix = pd.DataFrame([], columns=confusion_matrix_columns)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_model.predict(X_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    row = pd.DataFrame([np.zeros(len(categories), dtype=np.int64)], columns=confusion_matrix_columns, index=[category])\n",
    "    \n",
    "    examples = df.loc[df['category'] == category]\n",
    "    indices = examples.index\n",
    "    \n",
    "    pred = predictions[indices]\n",
    "   \n",
    "    for i in range(len(indices)):\n",
    "        predicted_category = categories[np.argmax(np.round(pred[i]))]\n",
    "        row[predicted_category] += 1\n",
    "        \n",
    "    if category_lengths[category] > 0:\n",
    "        row /= category_lengths[category]\n",
    "        \n",
    "    confusion_matrix = confusion_matrix.append(row)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix.infer_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "sn.heatmap(confusion_matrix, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
