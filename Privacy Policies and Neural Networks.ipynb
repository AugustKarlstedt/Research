{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network Classification of Privacy Policy Data Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## August Karlstedt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import imp\n",
    "import operator\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "#import pickle\n",
    "#from six.moves import urllib\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK\n",
    "import hyperopt\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras import metrics\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "#import fasttext\n",
    "# https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "\n",
    "import gensim\n",
    "# https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup the categories that we'll try to classify. We'll also need one-hot encodings that for the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the OPP paper https://www.usableprivacy.org/static/files/swilson_acl_2016.pdf:\n",
    "\n",
    "1. **First Party Collection/Use**: how and why a service provider collects user information.\n",
    "2. **Third Party Sharing/Collection**: how user information may be shared with or collected by third parties. \n",
    "3. **User Choice/Control**: choices and control options available to users. \n",
    "4. **User Access, Edit, & Deletion**: if and how users may access, edit, or delete their information. \n",
    "5. **Data Retention**: how long user information is stored. \n",
    "6. **Data Security**: how user information is protected. \n",
    "7. **Policy Change**: if and how users will be in formed about changes to the privacy policy. \n",
    "8. **Do Not Track**: if and how Do Not Track signals 3 for online tracking and advertising are honored. \n",
    "9. **International & Specific Audiences**: practices that pertain only to a specific group of users (e.g., children, Europeans, or California residents). \n",
    "10. **Other**: additional sublabels for introductory or general text, contact information, and practices not covered by the other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \n",
    "    files = []\n",
    "    data = []\n",
    "    header = ['Annotation ID', 'Batch ID', 'Annotator ID', 'Policy ID', 'Segment ID', 'Category Name', 'Attributes/Values', 'Policy URL', 'Date']\n",
    "    keep_columns = ['Segment ID', 'Category Name', 'Attributes/Values']\n",
    "    for file in glob.glob(\"data\\\\annotations/*.csv\"):\n",
    "        files.append(file[17:-4])\n",
    "        data.append(pd.read_csv(file, names=header)[keep_columns])\n",
    "        \n",
    "    policies = []\n",
    "    for file in files:\n",
    "        with open(\"data\\\\sanitized_policies/{}.html\".format(file)) as f:\n",
    "            policies.append(f.readlines()[0].split('|||'))\n",
    "\n",
    "    # categories = set()\n",
    "    # for datum in data:\n",
    "    #     cat = datum['Category Name']\n",
    "    #     categories.update(cat)\n",
    "    # categories\n",
    "\n",
    "    categories = [\n",
    "     'Data Retention', # 0\n",
    "     'Data Security', # 1\n",
    "     'Do Not Track', # 2\n",
    "     'First Party Collection/Use', # 3\n",
    "     'International and Specific Audiences', # 4\n",
    "     'Other', # 5\n",
    "     'Policy Change', # 6\n",
    "     'Third Party Sharing/Collection', # 7\n",
    "     'User Access, Edit and Deletion', # 8\n",
    "     'User Choice/Control', # 9\n",
    "     'None' # 10\n",
    "    ]\n",
    "\n",
    "    one_hot_categories = np.identity(len(categories))\n",
    "\n",
    "    cat_dict = {\n",
    "     categories[0]:  one_hot_categories[0],\n",
    "     categories[1]:  one_hot_categories[1],\n",
    "     categories[2]:  one_hot_categories[2],\n",
    "     categories[3]:  one_hot_categories[3],\n",
    "     categories[4]:  one_hot_categories[4],\n",
    "     categories[5]:  one_hot_categories[5],\n",
    "     categories[6]:  one_hot_categories[6],\n",
    "     categories[7]:  one_hot_categories[7],\n",
    "     categories[8]:  one_hot_categories[8],\n",
    "     categories[9]:  one_hot_categories[9],\n",
    "     categories[10]: one_hot_categories[10],\n",
    "    }\n",
    "\n",
    "    # attribute_value_types = set()\n",
    "    # attribute_value_values = set()\n",
    "    # for datum in data:\n",
    "    #     avs = datum['Attributes/Values']\n",
    "    #     for row in avs:\n",
    "    #         parsed = json.loads(row)\n",
    "    #         keys = list(parsed.keys())\n",
    "    #         attribute_value_types.update(keys)\n",
    "    #         for key in keys:\n",
    "    #             attribute_value_values.add(parsed[key]['value'])\n",
    "\n",
    "    attribute_value_types = ['Access Scope',\n",
    "     'Access Type',\n",
    "     'Action First-Party',\n",
    "     'Action Third Party',\n",
    "     'Audience Type',\n",
    "     'Change Type',\n",
    "     'Choice Scope',\n",
    "     'Choice Type',\n",
    "     'Collection Mode',\n",
    "     'Do Not Track policy',\n",
    "     'Does/Does Not',\n",
    "     'Identifiability',\n",
    "     'Notification Type',\n",
    "     'Other Type',\n",
    "     'Personal Information Type',\n",
    "     'Purpose',\n",
    "     'Retention Period',\n",
    "     'Retention Purpose',\n",
    "     'Security Measure',\n",
    "     'Third Party Entity',\n",
    "     'User Choice',\n",
    "     'User Type']\n",
    "\n",
    "    attribute_value_values = ['Additional service/feature',\n",
    "     'Advertising',\n",
    "     'Aggregated or anonymized',\n",
    "     'Analytics/Research',\n",
    "     'Basic service/feature',\n",
    "     'Both',\n",
    "     'Browser/device privacy controls',\n",
    "     'Californians',\n",
    "     'Children',\n",
    "     'Citizens from other countries',\n",
    "     'Collect from user on other websites',\n",
    "     'Collect in mobile app',\n",
    "     'Collect on first party website/app',\n",
    "     'Collect on mobile website',\n",
    "     'Collect on website',\n",
    "     'Collection',\n",
    "     'Computer information',\n",
    "     'Contact',\n",
    "     'Cookies and tracking elements',\n",
    "     'Data access limitation',\n",
    "     'Deactivate account',\n",
    "     'Delete account (full)',\n",
    "     'Delete account (partial)',\n",
    "     'Demographic',\n",
    "     'Does',\n",
    "     'Does Not',\n",
    "     'Dont use service/feature',\n",
    "     'Edit information',\n",
    "     'Europeans',\n",
    "     'Explicit',\n",
    "     'Export',\n",
    "     'Financial',\n",
    "     'First party collection',\n",
    "     'First party use',\n",
    "     'First-party privacy controls',\n",
    "     'General notice in privacy policy',\n",
    "     'General notice on website',\n",
    "     'Generic',\n",
    "     'Generic personal information',\n",
    "     'Health',\n",
    "     'Honored',\n",
    "     'IP address and device IDs',\n",
    "     'Identifiable',\n",
    "     'Implicit',\n",
    "     'In case of merger or acquisition',\n",
    "     'Indefinitely',\n",
    "     'Introductory/Generic',\n",
    "     'Legal requirement',\n",
    "     'Limited',\n",
    "     'Location',\n",
    "     'Marketing',\n",
    "     'Mentioned, but unclear if honored',\n",
    "     'Merger/Acquisition',\n",
    "     'Named third party',\n",
    "     'No notification',\n",
    "     'Non-privacy relevant change',\n",
    "     'None',\n",
    "     'Not honored',\n",
    "     'Not mentioned',\n",
    "     'Opt-in',\n",
    "     'Opt-out',\n",
    "     'Opt-out link',\n",
    "     'Opt-out via contacting company',\n",
    "     'Other',\n",
    "     'Other data about user',\n",
    "     'Other part of company/affiliate',\n",
    "     'Other users',\n",
    "     'Perform service',\n",
    "     'Personal identifier',\n",
    "     'Personal notice',\n",
    "     'Personalization/Customization',\n",
    "     'Practice not covered',\n",
    "     'Privacy contact information',\n",
    "     'Privacy relevant change',\n",
    "     'Privacy review/audit',\n",
    "     'Privacy training',\n",
    "     'Privacy/Security program',\n",
    "     'Profile data',\n",
    "     'Public',\n",
    "     'Receive from other parts of company/affiliates',\n",
    "     'Receive from other service/third-party (named)',\n",
    "     'Receive from other service/third-party (unnamed)',\n",
    "     'Receive/Shared with',\n",
    "     'Secure data storage',\n",
    "     'Secure data transfer',\n",
    "     'Secure user authentication',\n",
    "     'See',\n",
    "     'Service Operation and Security',\n",
    "     'Service operation and security',\n",
    "     'Social media data',\n",
    "     'Stated Period',\n",
    "     'Survey data',\n",
    "     'Third party sharing/collection',\n",
    "     'Third party use',\n",
    "     'Third-party privacy controls',\n",
    "     'Track on first party website/app',\n",
    "     'Track user on other websites',\n",
    "     'Transactional data',\n",
    "     'Unnamed third party',\n",
    "     'Unspecified',\n",
    "     'Use',\n",
    "     'User Profile',\n",
    "     'User account data',\n",
    "     'User online activities',\n",
    "     'User participation',\n",
    "     'User profile',\n",
    "     'User with account',\n",
    "     'User without account',\n",
    "     'View',\n",
    "     'not-selected']\n",
    "\n",
    "    model = gensim.models.Doc2Vec(vector_size=100)\n",
    "\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    chosen_categories = ['First Party Collection/Use', \n",
    "                         'Third Party Sharing/Collection', \n",
    "                         'Other', \n",
    "                         'User Choice/Control', \n",
    "                         'Data Security',\n",
    "                         'International and Specific Audiences',\n",
    "                         'User Access, Edit and Deletion',\n",
    "                         'Policy Change',\n",
    "                         'Data Retention',\n",
    "                         'Do Not Track',\n",
    "                         'None' # added by us, not in original corpus\n",
    "                        ]\n",
    "    remove_text = ['null', 'Not selected']\n",
    "\n",
    "    df_columns = ['text', 'category', 'category one hot', 'text vec']\n",
    "    df = pd.DataFrame([], columns=df_columns)\n",
    "    series = []\n",
    "    documents = []\n",
    "    cats = []\n",
    "\n",
    "    remove_spans = {} # dictionary of policy ids and list of start, stop tuples that are then removed\n",
    "    # remove_spans structure:\n",
    "\n",
    "    '''\n",
    "    {\n",
    "    \"2\": --> this is the policy id\n",
    "      {\n",
    "       \"6\": [(20, 30), (30, 50)], --> this is the segment id\n",
    "       \"8\": [(40, 123)] --> which maps to a list of tuple of start, end indices\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "\n",
    "\n",
    "    idx = 0\n",
    "    for datum_idx in range(len(data)):\n",
    "        datum = data[datum_idx]\n",
    "        for idx in range(len(datum)):        \n",
    "            category = datum['Category Name'][idx]\n",
    "\n",
    "            if chosen_categories is None:\n",
    "                continue\n",
    "\n",
    "            if category not in chosen_categories:\n",
    "                continue\n",
    "\n",
    "            segment_id = datum['Segment ID'][idx]\n",
    "            if datum_idx not in remove_spans:\n",
    "                remove_spans[datum_idx] = {}\n",
    "            if segment_id not in remove_spans[datum_idx]:\n",
    "                remove_spans[datum_idx][segment_id] = []\n",
    "\n",
    "            # ok, we have our policy text, now we need to \n",
    "            # remove all of the spans that are associated with a category\n",
    "            # so we can attribute that text to the 'None' category\n",
    "\n",
    "            parsed = json.loads(datum['Attributes/Values'][idx])\n",
    "            for value in attribute_value_types:\n",
    "                if value in parsed.keys():\n",
    "                    attributes = parsed[value]\n",
    "                    has_selected_text = 'selectedText' in attributes\n",
    "                    has_start_idx = 'startIndexInSegment' in attributes\n",
    "                    has_end_idx = 'endIndexInSegment' in attributes\n",
    "                    if has_selected_text and has_start_idx and has_end_idx:\n",
    "                        text = attributes['selectedText']\n",
    "                        start_idx = attributes['startIndexInSegment']\n",
    "                        end_idx = attributes['endIndexInSegment']\n",
    "\n",
    "                        if text in remove_text or start_idx == -1 or end_idx == -1:\n",
    "                            continue\n",
    "\n",
    "                        remove_spans[datum_idx][segment_id].append((start_idx, end_idx))\n",
    "\n",
    "                        text = text.lower()\n",
    "                        processed_text = word_tokenize(text)\n",
    "                        #processed_text = [stemmer.stem(word) for word in processed_text]\n",
    "                        processed_text = [lemmatizer.lemmatize(word) for word in processed_text]\n",
    "\n",
    "                        doc = gensim.models.doc2vec.TaggedDocument(processed_text, [idx])\n",
    "                        documents.append(doc)\n",
    "                        cats.append(cat_dict[category])\n",
    "                        text = ' '.join(processed_text)\n",
    "                        series.append(pd.Series([text, category, cat_dict[category], None], index=df_columns))\n",
    "\n",
    "                        idx += 1\n",
    "\n",
    "    SHOULD_PROCESS_NONE_CATEGORY = True\n",
    "\n",
    "    replace_items = [\"<br>\", \"<strong>\", \"</strong>\", \"<ul>\", \"</ul>\", \"<li>\", \"</li>\", \"<ol>\", \"</ol>\"]\n",
    "    category = 'None'\n",
    "    none_count = 0\n",
    "\n",
    "    if SHOULD_PROCESS_NONE_CATEGORY:\n",
    "        for policy_idx in remove_spans:\n",
    "            policy = policies[policy_idx]\n",
    "            for segment_idx in remove_spans[policy_idx]:\n",
    "                try:\n",
    "                    policy_segment = policy[segment_idx]\n",
    "                except IndexError as e:\n",
    "                    #print(e, policy_idx, segment_idx)\n",
    "                    continue\n",
    "                segment_text = policy_segment\n",
    "                for span in remove_spans[policy_idx][segment_idx]:\n",
    "                    start_idx = span[0]\n",
    "                    end_idx = span[1]\n",
    "                    segment_text = segment_text[:start_idx] + \" \" + segment_text[end_idx:]\n",
    "                segment_text = segment_text.lower()\n",
    "                for item in replace_items:\n",
    "                    segment_text = segment_text.replace(item, \" \")\n",
    "                segment_text = segment_text.strip()\n",
    "                if segment_text: # check if we have any characters at all\n",
    "                    processed_text = word_tokenize(segment_text)\n",
    "                    processed_text = [lemmatizer.lemmatize(word) for word in processed_text]\n",
    "\n",
    "                    doc = gensim.models.doc2vec.TaggedDocument(processed_text, [idx])\n",
    "                    documents.append(doc)\n",
    "                    cats.append(cat_dict[category])\n",
    "                    text = ' '.join(processed_text)\n",
    "                    series.append(pd.Series([text, category, cat_dict[category], None], index=df_columns))\n",
    "                    none_count += 1\n",
    "                    idx += 1\n",
    "\n",
    "        #print('None count: {}'.format(none_count))\n",
    "\n",
    "        cats = np.array(cats)\n",
    "\n",
    "        df = df.append(series, ignore_index=True)\n",
    "        #print(df.shape)\n",
    "\n",
    "        model.build_vocab(documents)\n",
    "        model.train(documents, total_examples=len(documents), epochs=16)\n",
    "\n",
    "        vecs = []\n",
    "        for row in df.itertuples():\n",
    "            category_not_chosen = chosen_categories is None\n",
    "            category_chosen_and_matches = chosen_categories is not None and row.category in chosen_categories\n",
    "            if category_chosen_and_matches or category_not_chosen:\n",
    "                model.random = np.random.RandomState(1234)\n",
    "                vecs.append(np.array(model.infer_vector(word_tokenize(row.text))))\n",
    "\n",
    "        vecs = np.array(vecs)\n",
    "        #print(vecs.shape)\n",
    "\n",
    "        return vecs, cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Since these Doc2Vec vectors are our only input data right now, let's just use them directly as our input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \n",
    "    vecs, cats = get_data()\n",
    "    \n",
    "    choice = np.random.choice(len(vecs), len(vecs), replace=False)\n",
    "    test_percentage = 0.25 # keep 25% of data for testing\n",
    "    test_amount = math.floor(0.25 * len(vecs))\n",
    "    train_indices = choice[test_amount:]\n",
    "    test_indices = choice[:test_amount]\n",
    "    \n",
    "    # vecs, cats\n",
    "    x_train = vecs[train_indices]\n",
    "    x_test = vecs[test_indices]\n",
    "    y_train = cats[train_indices]\n",
    "    y_test = cats[test_indices]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll use a simple neural network consisting of:\n",
    "\n",
    "1. **Fully connected** layer with 256 nodes, relu activation\n",
    "2. **Dropout** 25% of the inputs\n",
    "3. **Fully connected** layer with 256 nodes, relu activation\n",
    "4. **Dropout** 25% of the inputs\n",
    "5. **Fully connected** layer with 11 nodes, softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Dense( {{choice([32, 64, 128, 256, 512, 1024])}}, batch_input_shape=(None, 100, )))\n",
    "    nn_model.add(Activation( {{choice(['relu', 'tanh', 'sigmoid'])}} ))\n",
    "    \n",
    "    if conditional( {{choice(['dropout', 'no dropout'])}} ) == 'dropout':\n",
    "        nn_model.add(Dropout( {{uniform(0, 1)}} ))\n",
    "    \n",
    "    nn_model.add(Dense(11))\n",
    "    nn_model.add(Activation('softmax'))\n",
    "    \n",
    "    nn_model.compile(loss='categorical_crossentropy', optimizer={{choice(['sgd', 'rmsprop', 'adagrad', 'adam', 'nadam'])}}, metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    #print(nn_model.summary())\n",
    "\n",
    "    tensorboard_callback = TensorBoard(log_dir='C:/tmp/pp_run-'+time.strftime(\"%Y-%m-%d-%H%M%S\"))\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=4, verbose=1, mode='auto')\n",
    "    history = nn_model.fit(x_train, y_train, batch_size=128, epochs=16, verbose=2, validation_data=(x_test, y_test), callbacks=[tensorboard_callback])\n",
    "    acc = history.history['val_categorical_accuracy'][-1]\n",
    "    print('Test accuracy:', acc)\n",
    "    \n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': nn_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have TensorBoard running, you'll see it in the IFrame below!\n",
    "\n",
    "(With default settings, pointing to http://localhost:6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x221f020e550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame('http://localhost:6006', '100%', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import imp\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import operator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import math\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import glob\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from IPython.display import IFrame\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import nltk\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nltk.tokenize import word_tokenize\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sn\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import hyperopt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Dense, Activation, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM, TimeDistributed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping, TensorBoard\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import metrics\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils.np_utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import gensim\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [32, 64, 128, 256, 512, 1024]),\n",
      "        'Activation': hp.choice('Activation', ['relu', 'tanh', 'sigmoid']),\n",
      "        'conditional': hp.choice('conditional', ['dropout', 'no dropout']),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'optimizer': hp.choice('optimizer', ['sgd', 'rmsprop', 'adagrad', 'adam', 'nadam']),\n",
      "    }\n",
      "\n",
      ">>> Functions\n",
      "    1: def get_data():\n",
      "    2:     \n",
      "    3:     files = []\n",
      "    4:     data = []\n",
      "    5:     header = ['Annotation ID', 'Batch ID', 'Annotator ID', 'Policy ID', 'Segment ID', 'Category Name', 'Attributes/Values', 'Policy URL', 'Date']\n",
      "    6:     keep_columns = ['Segment ID', 'Category Name', 'Attributes/Values']\n",
      "    7:     for file in glob.glob(\"data\\\\annotations/*.csv\"):\n",
      "    8:         files.append(file[17:-4])\n",
      "    9:         data.append(pd.read_csv(file, names=header)[keep_columns])\n",
      "   10:         \n",
      "   11:     policies = []\n",
      "   12:     for file in files:\n",
      "   13:         with open(\"data\\\\sanitized_policies/{}.html\".format(file)) as f:\n",
      "   14:             policies.append(f.readlines()[0].split('|||'))\n",
      "   15: \n",
      "   16:     # categories = set()\n",
      "   17:     # for datum in data:\n",
      "   18:     #     cat = datum['Category Name']\n",
      "   19:     #     categories.update(cat)\n",
      "   20:     # categories\n",
      "   21: \n",
      "   22:     categories = [\n",
      "   23:      'Data Retention', # 0\n",
      "   24:      'Data Security', # 1\n",
      "   25:      'Do Not Track', # 2\n",
      "   26:      'First Party Collection/Use', # 3\n",
      "   27:      'International and Specific Audiences', # 4\n",
      "   28:      'Other', # 5\n",
      "   29:      'Policy Change', # 6\n",
      "   30:      'Third Party Sharing/Collection', # 7\n",
      "   31:      'User Access, Edit and Deletion', # 8\n",
      "   32:      'User Choice/Control', # 9\n",
      "   33:      'None' # 10\n",
      "   34:     ]\n",
      "   35: \n",
      "   36:     one_hot_categories = np.identity(len(categories))\n",
      "   37: \n",
      "   38:     cat_dict = {\n",
      "   39:      categories[0]:  one_hot_categories[0],\n",
      "   40:      categories[1]:  one_hot_categories[1],\n",
      "   41:      categories[2]:  one_hot_categories[2],\n",
      "   42:      categories[3]:  one_hot_categories[3],\n",
      "   43:      categories[4]:  one_hot_categories[4],\n",
      "   44:      categories[5]:  one_hot_categories[5],\n",
      "   45:      categories[6]:  one_hot_categories[6],\n",
      "   46:      categories[7]:  one_hot_categories[7],\n",
      "   47:      categories[8]:  one_hot_categories[8],\n",
      "   48:      categories[9]:  one_hot_categories[9],\n",
      "   49:      categories[10]: one_hot_categories[10],\n",
      "   50:     }\n",
      "   51: \n",
      "   52:     # attribute_value_types = set()\n",
      "   53:     # attribute_value_values = set()\n",
      "   54:     # for datum in data:\n",
      "   55:     #     avs = datum['Attributes/Values']\n",
      "   56:     #     for row in avs:\n",
      "   57:     #         parsed = json.loads(row)\n",
      "   58:     #         keys = list(parsed.keys())\n",
      "   59:     #         attribute_value_types.update(keys)\n",
      "   60:     #         for key in keys:\n",
      "   61:     #             attribute_value_values.add(parsed[key]['value'])\n",
      "   62: \n",
      "   63:     attribute_value_types = ['Access Scope',\n",
      "   64:      'Access Type',\n",
      "   65:      'Action First-Party',\n",
      "   66:      'Action Third Party',\n",
      "   67:      'Audience Type',\n",
      "   68:      'Change Type',\n",
      "   69:      'Choice Scope',\n",
      "   70:      'Choice Type',\n",
      "   71:      'Collection Mode',\n",
      "   72:      'Do Not Track policy',\n",
      "   73:      'Does/Does Not',\n",
      "   74:      'Identifiability',\n",
      "   75:      'Notification Type',\n",
      "   76:      'Other Type',\n",
      "   77:      'Personal Information Type',\n",
      "   78:      'Purpose',\n",
      "   79:      'Retention Period',\n",
      "   80:      'Retention Purpose',\n",
      "   81:      'Security Measure',\n",
      "   82:      'Third Party Entity',\n",
      "   83:      'User Choice',\n",
      "   84:      'User Type']\n",
      "   85: \n",
      "   86:     attribute_value_values = ['Additional service/feature',\n",
      "   87:      'Advertising',\n",
      "   88:      'Aggregated or anonymized',\n",
      "   89:      'Analytics/Research',\n",
      "   90:      'Basic service/feature',\n",
      "   91:      'Both',\n",
      "   92:      'Browser/device privacy controls',\n",
      "   93:      'Californians',\n",
      "   94:      'Children',\n",
      "   95:      'Citizens from other countries',\n",
      "   96:      'Collect from user on other websites',\n",
      "   97:      'Collect in mobile app',\n",
      "   98:      'Collect on first party website/app',\n",
      "   99:      'Collect on mobile website',\n",
      "  100:      'Collect on website',\n",
      "  101:      'Collection',\n",
      "  102:      'Computer information',\n",
      "  103:      'Contact',\n",
      "  104:      'Cookies and tracking elements',\n",
      "  105:      'Data access limitation',\n",
      "  106:      'Deactivate account',\n",
      "  107:      'Delete account (full)',\n",
      "  108:      'Delete account (partial)',\n",
      "  109:      'Demographic',\n",
      "  110:      'Does',\n",
      "  111:      'Does Not',\n",
      "  112:      'Dont use service/feature',\n",
      "  113:      'Edit information',\n",
      "  114:      'Europeans',\n",
      "  115:      'Explicit',\n",
      "  116:      'Export',\n",
      "  117:      'Financial',\n",
      "  118:      'First party collection',\n",
      "  119:      'First party use',\n",
      "  120:      'First-party privacy controls',\n",
      "  121:      'General notice in privacy policy',\n",
      "  122:      'General notice on website',\n",
      "  123:      'Generic',\n",
      "  124:      'Generic personal information',\n",
      "  125:      'Health',\n",
      "  126:      'Honored',\n",
      "  127:      'IP address and device IDs',\n",
      "  128:      'Identifiable',\n",
      "  129:      'Implicit',\n",
      "  130:      'In case of merger or acquisition',\n",
      "  131:      'Indefinitely',\n",
      "  132:      'Introductory/Generic',\n",
      "  133:      'Legal requirement',\n",
      "  134:      'Limited',\n",
      "  135:      'Location',\n",
      "  136:      'Marketing',\n",
      "  137:      'Mentioned, but unclear if honored',\n",
      "  138:      'Merger/Acquisition',\n",
      "  139:      'Named third party',\n",
      "  140:      'No notification',\n",
      "  141:      'Non-privacy relevant change',\n",
      "  142:      'None',\n",
      "  143:      'Not honored',\n",
      "  144:      'Not mentioned',\n",
      "  145:      'Opt-in',\n",
      "  146:      'Opt-out',\n",
      "  147:      'Opt-out link',\n",
      "  148:      'Opt-out via contacting company',\n",
      "  149:      'Other',\n",
      "  150:      'Other data about user',\n",
      "  151:      'Other part of company/affiliate',\n",
      "  152:      'Other users',\n",
      "  153:      'Perform service',\n",
      "  154:      'Personal identifier',\n",
      "  155:      'Personal notice',\n",
      "  156:      'Personalization/Customization',\n",
      "  157:      'Practice not covered',\n",
      "  158:      'Privacy contact information',\n",
      "  159:      'Privacy relevant change',\n",
      "  160:      'Privacy review/audit',\n",
      "  161:      'Privacy training',\n",
      "  162:      'Privacy/Security program',\n",
      "  163:      'Profile data',\n",
      "  164:      'Public',\n",
      "  165:      'Receive from other parts of company/affiliates',\n",
      "  166:      'Receive from other service/third-party (named)',\n",
      "  167:      'Receive from other service/third-party (unnamed)',\n",
      "  168:      'Receive/Shared with',\n",
      "  169:      'Secure data storage',\n",
      "  170:      'Secure data transfer',\n",
      "  171:      'Secure user authentication',\n",
      "  172:      'See',\n",
      "  173:      'Service Operation and Security',\n",
      "  174:      'Service operation and security',\n",
      "  175:      'Social media data',\n",
      "  176:      'Stated Period',\n",
      "  177:      'Survey data',\n",
      "  178:      'Third party sharing/collection',\n",
      "  179:      'Third party use',\n",
      "  180:      'Third-party privacy controls',\n",
      "  181:      'Track on first party website/app',\n",
      "  182:      'Track user on other websites',\n",
      "  183:      'Transactional data',\n",
      "  184:      'Unnamed third party',\n",
      "  185:      'Unspecified',\n",
      "  186:      'Use',\n",
      "  187:      'User Profile',\n",
      "  188:      'User account data',\n",
      "  189:      'User online activities',\n",
      "  190:      'User participation',\n",
      "  191:      'User profile',\n",
      "  192:      'User with account',\n",
      "  193:      'User without account',\n",
      "  194:      'View',\n",
      "  195:      'not-selected']\n",
      "  196: \n",
      "  197:     model = gensim.models.Doc2Vec(vector_size=100)\n",
      "  198: \n",
      "  199:     stemmer = nltk.stem.porter.PorterStemmer()\n",
      "  200:     lemmatizer = nltk.stem.WordNetLemmatizer()\n",
      "  201: \n",
      "  202:     chosen_categories = ['First Party Collection/Use', \n",
      "  203:                          'Third Party Sharing/Collection', \n",
      "  204:                          'Other', \n",
      "  205:                          'User Choice/Control', \n",
      "  206:                          'Data Security',\n",
      "  207:                          'International and Specific Audiences',\n",
      "  208:                          'User Access, Edit and Deletion',\n",
      "  209:                          'Policy Change',\n",
      "  210:                          'Data Retention',\n",
      "  211:                          'Do Not Track',\n",
      "  212:                          'None' # added by us, not in original corpus\n",
      "  213:                         ]\n",
      "  214:     remove_text = ['null', 'Not selected']\n",
      "  215: \n",
      "  216:     df_columns = ['text', 'category', 'category one hot', 'text vec']\n",
      "  217:     df = pd.DataFrame([], columns=df_columns)\n",
      "  218:     series = []\n",
      "  219:     documents = []\n",
      "  220:     cats = []\n",
      "  221: \n",
      "  222:     remove_spans = {} # dictionary of policy ids and list of start, stop tuples that are then removed\n",
      "  223:     # remove_spans structure:\n",
      "  224: \n",
      "  225:     '''\n",
      "  226:     {\n",
      "  227:     \"2\": --> this is the policy id\n",
      "  228:       {\n",
      "  229:        \"6\": [(20, 30), (30, 50)], --> this is the segment id\n",
      "  230:        \"8\": [(40, 123)] --> which maps to a list of tuple of start, end indices\n",
      "  231:       }\n",
      "  232:     }\n",
      "  233:     '''\n",
      "  234: \n",
      "  235: \n",
      "  236:     idx = 0\n",
      "  237:     for datum_idx in range(len(data)):\n",
      "  238:         datum = data[datum_idx]\n",
      "  239:         for idx in range(len(datum)):        \n",
      "  240:             category = datum['Category Name'][idx]\n",
      "  241: \n",
      "  242:             if chosen_categories is None:\n",
      "  243:                 continue\n",
      "  244: \n",
      "  245:             if category not in chosen_categories:\n",
      "  246:                 continue\n",
      "  247: \n",
      "  248:             segment_id = datum['Segment ID'][idx]\n",
      "  249:             if datum_idx not in remove_spans:\n",
      "  250:                 remove_spans[datum_idx] = {}\n",
      "  251:             if segment_id not in remove_spans[datum_idx]:\n",
      "  252:                 remove_spans[datum_idx][segment_id] = []\n",
      "  253: \n",
      "  254:             # ok, we have our policy text, now we need to \n",
      "  255:             # remove all of the spans that are associated with a category\n",
      "  256:             # so we can attribute that text to the 'None' category\n",
      "  257: \n",
      "  258:             parsed = json.loads(datum['Attributes/Values'][idx])\n",
      "  259:             for value in attribute_value_types:\n",
      "  260:                 if value in parsed.keys():\n",
      "  261:                     attributes = parsed[value]\n",
      "  262:                     has_selected_text = 'selectedText' in attributes\n",
      "  263:                     has_start_idx = 'startIndexInSegment' in attributes\n",
      "  264:                     has_end_idx = 'endIndexInSegment' in attributes\n",
      "  265:                     if has_selected_text and has_start_idx and has_end_idx:\n",
      "  266:                         text = attributes['selectedText']\n",
      "  267:                         start_idx = attributes['startIndexInSegment']\n",
      "  268:                         end_idx = attributes['endIndexInSegment']\n",
      "  269: \n",
      "  270:                         if text in remove_text or start_idx == -1 or end_idx == -1:\n",
      "  271:                             continue\n",
      "  272: \n",
      "  273:                         remove_spans[datum_idx][segment_id].append((start_idx, end_idx))\n",
      "  274: \n",
      "  275:                         text = text.lower()\n",
      "  276:                         processed_text = word_tokenize(text)\n",
      "  277:                         #processed_text = [stemmer.stem(word) for word in processed_text]\n",
      "  278:                         processed_text = [lemmatizer.lemmatize(word) for word in processed_text]\n",
      "  279: \n",
      "  280:                         doc = gensim.models.doc2vec.TaggedDocument(processed_text, [idx])\n",
      "  281:                         documents.append(doc)\n",
      "  282:                         cats.append(cat_dict[category])\n",
      "  283:                         text = ' '.join(processed_text)\n",
      "  284:                         series.append(pd.Series([text, category, cat_dict[category], None], index=df_columns))\n",
      "  285: \n",
      "  286:                         idx += 1\n",
      "  287: \n",
      "  288:     SHOULD_PROCESS_NONE_CATEGORY = True\n",
      "  289: \n",
      "  290:     replace_items = [\"<br>\", \"<strong>\", \"</strong>\", \"<ul>\", \"</ul>\", \"<li>\", \"</li>\", \"<ol>\", \"</ol>\"]\n",
      "  291:     category = 'None'\n",
      "  292:     none_count = 0\n",
      "  293: \n",
      "  294:     if SHOULD_PROCESS_NONE_CATEGORY:\n",
      "  295:         for policy_idx in remove_spans:\n",
      "  296:             policy = policies[policy_idx]\n",
      "  297:             for segment_idx in remove_spans[policy_idx]:\n",
      "  298:                 try:\n",
      "  299:                     policy_segment = policy[segment_idx]\n",
      "  300:                 except IndexError as e:\n",
      "  301:                     #print(e, policy_idx, segment_idx)\n",
      "  302:                     continue\n",
      "  303:                 segment_text = policy_segment\n",
      "  304:                 for span in remove_spans[policy_idx][segment_idx]:\n",
      "  305:                     start_idx = span[0]\n",
      "  306:                     end_idx = span[1]\n",
      "  307:                     segment_text = segment_text[:start_idx] + \" \" + segment_text[end_idx:]\n",
      "  308:                 segment_text = segment_text.lower()\n",
      "  309:                 for item in replace_items:\n",
      "  310:                     segment_text = segment_text.replace(item, \" \")\n",
      "  311:                 segment_text = segment_text.strip()\n",
      "  312:                 if segment_text: # check if we have any characters at all\n",
      "  313:                     processed_text = word_tokenize(segment_text)\n",
      "  314:                     processed_text = [lemmatizer.lemmatize(word) for word in processed_text]\n",
      "  315: \n",
      "  316:                     doc = gensim.models.doc2vec.TaggedDocument(processed_text, [idx])\n",
      "  317:                     documents.append(doc)\n",
      "  318:                     cats.append(cat_dict[category])\n",
      "  319:                     text = ' '.join(processed_text)\n",
      "  320:                     series.append(pd.Series([text, category, cat_dict[category], None], index=df_columns))\n",
      "  321:                     none_count += 1\n",
      "  322:                     idx += 1\n",
      "  323: \n",
      "  324:         #print('None count: {}'.format(none_count))\n",
      "  325: \n",
      "  326:         cats = np.array(cats)\n",
      "  327: \n",
      "  328:         df = df.append(series, ignore_index=True)\n",
      "  329:         #print(df.shape)\n",
      "  330: \n",
      "  331:         model.build_vocab(documents)\n",
      "  332:         model.train(documents, total_examples=len(documents), epochs=16)\n",
      "  333: \n",
      "  334:         vecs = []\n",
      "  335:         for row in df.itertuples():\n",
      "  336:             category_not_chosen = chosen_categories is None\n",
      "  337:             category_chosen_and_matches = chosen_categories is not None and row.category in chosen_categories\n",
      "  338:             if category_chosen_and_matches or category_not_chosen:\n",
      "  339:                 model.random = np.random.RandomState(1234)\n",
      "  340:                 vecs.append(np.array(model.infer_vector(word_tokenize(row.text))))\n",
      "  341: \n",
      "  342:         vecs = np.array(vecs)\n",
      "  343:         #print(vecs.shape)\n",
      "  344: \n",
      "  345:         return vecs, cats\n",
      "  346: \n",
      "  347: \n",
      ">>> Data\n",
      "  1: \n",
      "  2: \n",
      "  3: vecs, cats = get_data()\n",
      "  4: \n",
      "  5: choice = np.random.choice(len(vecs), len(vecs), replace=False)\n",
      "  6: test_percentage = 0.25 # keep 25% of data for testing\n",
      "  7: test_amount = math.floor(0.25 * len(vecs))\n",
      "  8: train_indices = choice[test_amount:]\n",
      "  9: test_indices = choice[:test_amount]\n",
      " 10: \n",
      " 11: # vecs, cats\n",
      " 12: x_train = vecs[train_indices]\n",
      " 13: x_test = vecs[test_indices]\n",
      " 14: y_train = cats[train_indices]\n",
      " 15: y_test = cats[test_indices]\n",
      " 16: \n",
      " 17: \n",
      " 18: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3: \n",
      "   4:     nn_model = Sequential()\n",
      "   5:     nn_model.add(Dense( space['Dense'], batch_input_shape=(None, 100, )))\n",
      "   6:     nn_model.add(Activation( space['Activation'] ))\n",
      "   7:     \n",
      "   8:     if conditional( space['conditional'] ) == 'dropout':\n",
      "   9:         nn_model.add(Dropout( space['Dropout'] ))\n",
      "  10:     \n",
      "  11:     nn_model.add(Dense(11))\n",
      "  12:     nn_model.add(Activation('softmax'))\n",
      "  13:     \n",
      "  14:     nn_model.compile(loss='categorical_crossentropy', optimizer=space['optimizer'], metrics=[metrics.categorical_accuracy])\n",
      "  15: \n",
      "  16:     #print(nn_model.summary())\n",
      "  17: \n",
      "  18:     tensorboard_callback = TensorBoard(log_dir='C:/tmp/pp_run-'+time.strftime(\"%Y-%m-%d-%H%M%S\"))\n",
      "  19:     early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=4, verbose=1, mode='auto')\n",
      "  20:     history = nn_model.fit(x_train, y_train, batch_size=128, epochs=1, verbose=2, validation_data=(x_test, y_test), callbacks=[tensorboard_callback])\n",
      "  21:     acc = history.history['val_categorical_accuracy'][-1]\n",
      "  22:     print('Test accuracy:', acc)\n",
      "  23:     \n",
      "  24:     return {'loss': -acc, 'status': STATUS_OK, 'model': nn_model}\n",
      "  25: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59753 samples, validate on 19917 samples\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.5324 - categorical_accuracy: 0.4555 - val_loss: 1.3743 - val_categorical_accuracy: 0.4682\n",
      "Test accuracy: 0.4681930009988487\n",
      "Train on 59753 samples, validate on 19917 samples\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.5517 - categorical_accuracy: 0.4460 - val_loss: 1.3739 - val_categorical_accuracy: 0.4663\n",
      "Test accuracy: 0.46633529154035874\n",
      "Train on 59753 samples, validate on 19917 samples\n",
      "Epoch 1/1\n",
      " - 3s - loss: 1.5727 - categorical_accuracy: 0.4214 - val_loss: 1.3901 - val_categorical_accuracy: 0.4640\n",
      "Test accuracy: 0.4639754983808655\n",
      "Train on 59753 samples, validate on 19917 samples\n",
      "Epoch 1/1\n",
      " - 1s - loss: 1.5313 - categorical_accuracy: 0.4550 - val_loss: 1.3713 - val_categorical_accuracy: 0.4760\n",
      "Test accuracy: 0.4759752975294507\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c4832f64b70a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbest_run\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhyperopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Privacy Policies and Neural Networks'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8d708f37b3c0>\u001b[0m in \u001b[0;36mdata\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mchoice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-cec4023a6a1c>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\\\\annotations/*.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeep_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mpolicies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1085\u001b[0m             \u001b[0mnew_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1087\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1088\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnew_rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    328\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[0;32m    329\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[1;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m   6171\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6173\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   4635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4637\u001b[1;33m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4638\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4639\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mform_blocks\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   4659\u001b[0m     \u001b[0mnames_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4660\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnames_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4661\u001b[1;33m         \u001b[0mnames_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4662\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4663\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mnames_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "functions=[get_data]\n",
    "best_run, best_model = optim.minimize(model=create_model, data=data, functions=functions, algo=hyperopt.rand.suggest, max_evals=4, trials=Trials(), notebook_name='Privacy Policies and Neural Networks')\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a confusion matrix to see the accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, just to get some stats on each category, the number of items in each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_lengths = {}\n",
    "\n",
    "for category in categories:\n",
    "    category_lengths[category] = len(df.loc[df['category'] == category])\n",
    "    print(category_lengths[category], 'examples in the', category, 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate our confusion matrix, we need the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix_columns = categories\n",
    "\n",
    "confusion_matrix = pd.DataFrame([], columns=confusion_matrix_columns)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_model.predict(X_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    row = pd.DataFrame([np.zeros(len(categories), dtype=np.int64)], columns=confusion_matrix_columns, index=[category])\n",
    "    \n",
    "    examples = df.loc[df['category'] == category]\n",
    "    indices = examples.index\n",
    "    \n",
    "    pred = predictions[indices]\n",
    "   \n",
    "    for i in range(len(indices)):\n",
    "        predicted_category = categories[np.argmax(np.round(pred[i]))]\n",
    "        row[predicted_category] += 1\n",
    "        \n",
    "    if category_lengths[category] > 0:\n",
    "        row /= category_lengths[category]\n",
    "        \n",
    "    confusion_matrix = confusion_matrix.append(row)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix.infer_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "sn.heatmap(confusion_matrix, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
