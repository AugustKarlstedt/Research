{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network Classification of Privacy Policy Data Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## August Karlstedt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import imp\n",
    "import operator\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "#import pickle\n",
    "#from six.moves import urllib\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK\n",
    "import hyperopt\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras import metrics\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "#import fasttext\n",
    "# https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "\n",
    "import gensim\n",
    "# https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup the categories that we'll try to classify. We'll also need one-hot encodings that for the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the OPP paper https://www.usableprivacy.org/static/files/swilson_acl_2016.pdf:\n",
    "\n",
    "1. **First Party Collection/Use**: how and why a service provider collects user information.\n",
    "2. **Third Party Sharing/Collection**: how user information may be shared with or collected by third parties. \n",
    "3. **User Choice/Control**: choices and control options available to users. \n",
    "4. **User Access, Edit, & Deletion**: if and how users may access, edit, or delete their information. \n",
    "5. **Data Retention**: how long user information is stored. \n",
    "6. **Data Security**: how user information is protected. \n",
    "7. **Policy Change**: if and how users will be in formed about changes to the privacy policy. \n",
    "8. **Do Not Track**: if and how Do Not Track signals 3 for online tracking and advertising are honored. \n",
    "9. **International & Specific Audiences**: practices that pertain only to a specific group of users (e.g., children, Europeans, or California residents). \n",
    "10. **Other**: additional sublabels for introductory or general text, contact information, and practices not covered by the other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations():\n",
    "    filenames = []\n",
    "    annotations = []\n",
    "    \n",
    "    header = ['Annotation ID', 'Batch ID', 'Annotator ID', 'Policy ID', 'Segment ID', 'Category Name', 'Attributes/Values', 'Policy URL', 'Date']\n",
    "    keep_columns = ['Segment ID', 'Category Name', 'Attributes/Values']\n",
    "    \n",
    "    for file in glob.glob(\"data\\\\annotations/*.csv\"):\n",
    "        filenames.append(file[17:-4])\n",
    "        annotations.append(pd.read_csv(file, names=header)[keep_columns])\n",
    "    \n",
    "    return filenames, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sanitized_policies(annotation_files):\n",
    "    sanitized_policies = []\n",
    "    \n",
    "    for file in annotation_files:\n",
    "        with open(\"data\\\\sanitized_policies/{}.html\".format(file)) as f:\n",
    "            sanitized_policies.append(f.readlines()[0].split('|||'))\n",
    "            \n",
    "    return sanitized_policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories():\n",
    "    categories = [\n",
    "     'Data Retention', # 0\n",
    "     'Data Security', # 1\n",
    "     'Do Not Track', # 2\n",
    "     'First Party Collection/Use', # 3\n",
    "     'International and Specific Audiences', # 4\n",
    "     'Other', # 5\n",
    "     'Policy Change', # 6\n",
    "     'Third Party Sharing/Collection', # 7\n",
    "     'User Access, Edit and Deletion', # 8\n",
    "     'User Choice/Control', # 9\n",
    "     'None' # 10\n",
    "    ]\n",
    "    \n",
    "    categories_one_hot = np.identity(len(categories))\n",
    "\n",
    "    category_lookup_table = {\n",
    "     categories[0]:  categories_one_hot[0],\n",
    "     categories[1]:  categories_one_hot[1],\n",
    "     categories[2]:  categories_one_hot[2],\n",
    "     categories[3]:  categories_one_hot[3],\n",
    "     categories[4]:  categories_one_hot[4],\n",
    "     categories[5]:  categories_one_hot[5],\n",
    "     categories[6]:  categories_one_hot[6],\n",
    "     categories[7]:  categories_one_hot[7],\n",
    "     categories[8]:  categories_one_hot[8],\n",
    "     categories[9]:  categories_one_hot[9],\n",
    "     categories[10]: categories_one_hot[10],\n",
    "    }\n",
    "    \n",
    "    return categories, categories_one_hot, category_lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes():\n",
    "    \n",
    "    attribute_value_types = ['Access Scope',\n",
    "     'Access Type',\n",
    "     'Action First-Party',\n",
    "     'Action Third Party',\n",
    "     'Audience Type',\n",
    "     'Change Type',\n",
    "     'Choice Scope',\n",
    "     'Choice Type',\n",
    "     'Collection Mode',\n",
    "     'Do Not Track policy',\n",
    "     'Does/Does Not',\n",
    "     'Identifiability',\n",
    "     'Notification Type',\n",
    "     'Other Type',\n",
    "     'Personal Information Type',\n",
    "     'Purpose',\n",
    "     'Retention Period',\n",
    "     'Retention Purpose',\n",
    "     'Security Measure',\n",
    "     'Third Party Entity',\n",
    "     'User Choice',\n",
    "     'User Type'\n",
    "    ]\n",
    "\n",
    "    attribute_value_values = ['Additional service/feature',\n",
    "     'Advertising',\n",
    "     'Aggregated or anonymized',\n",
    "     'Analytics/Research',\n",
    "     'Basic service/feature',\n",
    "     'Both',\n",
    "     'Browser/device privacy controls',\n",
    "     'Californians',\n",
    "     'Children',\n",
    "     'Citizens from other countries',\n",
    "     'Collect from user on other websites',\n",
    "     'Collect in mobile app',\n",
    "     'Collect on first party website/app',\n",
    "     'Collect on mobile website',\n",
    "     'Collect on website',\n",
    "     'Collection',\n",
    "     'Computer information',\n",
    "     'Contact',\n",
    "     'Cookies and tracking elements',\n",
    "     'Data access limitation',\n",
    "     'Deactivate account',\n",
    "     'Delete account (full)',\n",
    "     'Delete account (partial)',\n",
    "     'Demographic',\n",
    "     'Does',\n",
    "     'Does Not',\n",
    "     'Dont use service/feature',\n",
    "     'Edit information',\n",
    "     'Europeans',\n",
    "     'Explicit',\n",
    "     'Export',\n",
    "     'Financial',\n",
    "     'First party collection',\n",
    "     'First party use',\n",
    "     'First-party privacy controls',\n",
    "     'General notice in privacy policy',\n",
    "     'General notice on website',\n",
    "     'Generic',\n",
    "     'Generic personal information',\n",
    "     'Health',\n",
    "     'Honored',\n",
    "     'IP address and device IDs',\n",
    "     'Identifiable',\n",
    "     'Implicit',\n",
    "     'In case of merger or acquisition',\n",
    "     'Indefinitely',\n",
    "     'Introductory/Generic',\n",
    "     'Legal requirement',\n",
    "     'Limited',\n",
    "     'Location',\n",
    "     'Marketing',\n",
    "     'Mentioned, but unclear if honored',\n",
    "     'Merger/Acquisition',\n",
    "     'Named third party',\n",
    "     'No notification',\n",
    "     'Non-privacy relevant change',\n",
    "     'None',\n",
    "     'Not honored',\n",
    "     'Not mentioned',\n",
    "     'Opt-in',\n",
    "     'Opt-out',\n",
    "     'Opt-out link',\n",
    "     'Opt-out via contacting company',\n",
    "     'Other',\n",
    "     'Other data about user',\n",
    "     'Other part of company/affiliate',\n",
    "     'Other users',\n",
    "     'Perform service',\n",
    "     'Personal identifier',\n",
    "     'Personal notice',\n",
    "     'Personalization/Customization',\n",
    "     'Practice not covered',\n",
    "     'Privacy contact information',\n",
    "     'Privacy relevant change',\n",
    "     'Privacy review/audit',\n",
    "     'Privacy training',\n",
    "     'Privacy/Security program',\n",
    "     'Profile data',\n",
    "     'Public',\n",
    "     'Receive from other parts of company/affiliates',\n",
    "     'Receive from other service/third-party (named)',\n",
    "     'Receive from other service/third-party (unnamed)',\n",
    "     'Receive/Shared with',\n",
    "     'Secure data storage',\n",
    "     'Secure data transfer',\n",
    "     'Secure user authentication',\n",
    "     'See',\n",
    "     'Service Operation and Security',\n",
    "     'Service operation and security',\n",
    "     'Social media data',\n",
    "     'Stated Period',\n",
    "     'Survey data',\n",
    "     'Third party sharing/collection',\n",
    "     'Third party use',\n",
    "     'Third-party privacy controls',\n",
    "     'Track on first party website/app',\n",
    "     'Track user on other websites',\n",
    "     'Transactional data',\n",
    "     'Unnamed third party',\n",
    "     'Unspecified',\n",
    "     'Use',\n",
    "     'User Profile',\n",
    "     'User account data',\n",
    "     'User online activities',\n",
    "     'User participation',\n",
    "     'User profile',\n",
    "     'User with account',\n",
    "     'User without account',\n",
    "     'View',\n",
    "     'not-selected'\n",
    "    ]\n",
    "    \n",
    "    return attribute_value_types, attribute_value_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chosen_categories():\n",
    "    return ['First Party Collection/Use', \n",
    "            'Third Party Sharing/Collection', \n",
    "            'Other', \n",
    "            'User Choice/Control', \n",
    "            'Data Security',\n",
    "            'International and Specific Audiences',\n",
    "            'User Access, Edit and Deletion',\n",
    "            'Policy Change',\n",
    "            'Data Retention',\n",
    "            'Do Not Track',\n",
    "            'None' # added by us, not in original corpus\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_to_remove():\n",
    "    return ['null', 'Not selected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_data(annotations, sanitized_policies, category_lookup_table):\n",
    "    chosen_categories = get_chosen_categories()\n",
    "    attribute_value_types, attribute_value_values = get_attributes()\n",
    "    remove_text = get_text_to_remove()\n",
    "    \n",
    "    #stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    documents = []\n",
    "    categories = []\n",
    "    \n",
    "    remove_spans = {} # dictionary of policy ids and list of start, stop tuples that are then removed\n",
    "\n",
    "    '''\n",
    "    remove_spans structure:\n",
    "    \n",
    "    {\n",
    "    \"2\": --> this is the policy id\n",
    "      {\n",
    "       \"6\": [(20, 30), (30, 50)], --> this is the segment id\n",
    "       \"8\": [(40, 123)] --> which maps to a list of tuple of start, end indices\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "        \n",
    "    ##  first process the annotations\n",
    "    for annotation_idx in range(len(annotations)):\n",
    "        annotation = annotations[annotation_idx]\n",
    "        \n",
    "        for idx in range(len(annotation)):        \n",
    "            category = annotation['Category Name'][idx]\n",
    "\n",
    "            if chosen_categories is None:\n",
    "                continue\n",
    "\n",
    "            if category not in chosen_categories:\n",
    "                continue\n",
    "\n",
    "            segment_id = annotation['Segment ID'][idx]\n",
    "            \n",
    "            if annotation_idx not in remove_spans:\n",
    "                remove_spans[annotation_idx] = {}\n",
    "                \n",
    "            if segment_id not in remove_spans[annotation_idx]:\n",
    "                remove_spans[annotation_idx][segment_id] = []\n",
    "\n",
    "            # ok, we have our policy text, now we need to \n",
    "            # remove all of the spans that are associated with a category\n",
    "            # so we can attribute that text to the 'None' category\n",
    "\n",
    "            parsed = json.loads(annotation['Attributes/Values'][idx])\n",
    "            for value in attribute_value_types:\n",
    "                if value not in parsed.keys():\n",
    "                    continue\n",
    "\n",
    "                attributes = parsed[value]\n",
    "                \n",
    "                if 'selectedText' not in attributes:\n",
    "                    continue\n",
    "                    \n",
    "                if 'startIndexInSegment' not in attributes:\n",
    "                    continue\n",
    "                    \n",
    "                if 'endIndexInSegment' not in attributes:\n",
    "                    continue\n",
    "\n",
    "                text = attributes['selectedText']\n",
    "                if text in remove_text:\n",
    "                    continue\n",
    "                    \n",
    "                start_idx = attributes['startIndexInSegment']\n",
    "                if start_idx == -1:\n",
    "                    continue\n",
    "                    \n",
    "                end_idx = attributes['endIndexInSegment']\n",
    "                if end_idx == -1:\n",
    "                    continue\n",
    "\n",
    "                remove_spans[annotation_idx][segment_id].append((start_idx, end_idx))\n",
    "\n",
    "                text = text.lower()\n",
    "                processed_text = word_tokenize(text)\n",
    "                processed_text = [lemmatizer.lemmatize(word) for word in processed_text]\n",
    "\n",
    "                doc = gensim.models.doc2vec.TaggedDocument(processed_text, [idx])\n",
    "                documents.append(doc)\n",
    "                \n",
    "                categories.append(category_lookup_table[category])\n",
    "                \n",
    "                idx += 1\n",
    "    \n",
    "    ## now process the remove spans from the policies\n",
    "    if 'None' in chosen_categories:\n",
    "        replace_items = [\"<br>\", \"<strong>\", \"</strong>\", \"<ul>\", \"</ul>\", \"<li>\", \"</li>\", \"<ol>\", \"</ol>\"]\n",
    "\n",
    "        for policy_idx in remove_spans:\n",
    "            policy = sanitized_policies[policy_idx]\n",
    "\n",
    "            for segment_idx in remove_spans[policy_idx]:\n",
    "\n",
    "                try:\n",
    "                    policy_segment = policy[segment_idx]\n",
    "                except IndexError as e:\n",
    "                    #print(e, policy_idx, segment_idx)\n",
    "                    continue\n",
    "\n",
    "                segment_text = policy_segment\n",
    "                for span in remove_spans[policy_idx][segment_idx]:\n",
    "                    start_idx = span[0]\n",
    "                    end_idx = span[1]\n",
    "                    segment_text = segment_text[:start_idx] + \" \" + segment_text[end_idx:]\n",
    "\n",
    "                segment_text = segment_text.lower()\n",
    "\n",
    "                for item in replace_items:\n",
    "                    segment_text = segment_text.replace(item, \" \")\n",
    "\n",
    "                segment_text = segment_text.strip()\n",
    "\n",
    "                if not segment_text: # check if we have any characters at all\n",
    "                    continue\n",
    "\n",
    "                processed_text = word_tokenize(segment_text)\n",
    "                processed_text = [lemmatizer.lemmatize(word) for word in processed_text]\n",
    "\n",
    "                doc = gensim.models.doc2vec.TaggedDocument(processed_text, [idx])\n",
    "                documents.append(doc)\n",
    "\n",
    "                categories.append(category_lookup_table['None'])\n",
    "\n",
    "                idx += 1\n",
    "    \n",
    "    return documents, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embeddings(documents, epochs=16):\n",
    "    model = gensim.models.Doc2Vec(vector_size=100)\n",
    "    model.build_vocab(documents)\n",
    "    model.train(documents, total_examples=len(documents), epochs=epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(model, text):\n",
    "    # set this so it's deterministic\n",
    "    model.random = np.random.RandomState(1234)\n",
    "    return model.infer_vector(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    filenames, annotations = get_annotations()\n",
    "    sanitized_policies = get_sanitized_policies(filenames)\n",
    "    categories, categories_one_hot, category_lookup_table = get_categories()\n",
    "    \n",
    "    documents, categories = get_annotation_data(annotations, sanitized_policies, category_lookup_table)\n",
    "    embedding_model = train_embeddings(documents)\n",
    "    \n",
    "    text_span_vectors = []\n",
    "    for idx in range(len(categories)):\n",
    "        text = ' '.join(documents[idx].words)\n",
    "        vector = get_vector(embedding_model, text)\n",
    "        text_span_vectors.append(vector)\n",
    "\n",
    "    return text_span_vectors, categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Since these Doc2Vec vectors are our only input data right now, let's just use them directly as our input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    text_span_vectors, categories = get_data()\n",
    "    \n",
    "    text_span_vectors = np.array(text_span_vectors)\n",
    "    categories = np.array(categories)\n",
    "    \n",
    "    choice = np.random.choice(len(text_span_vectors), len(text_span_vectors), replace=False)\n",
    "    test_percentage = 0.25 # keep 25% of data for testing\n",
    "    test_amount = math.floor(0.25 * len(text_span_vectors))\n",
    "    train_indices = np.array(choice[test_amount:])\n",
    "    test_indices = np.array(choice[:test_amount])\n",
    "    \n",
    "    x_train = text_span_vectors[train_indices]\n",
    "    x_test = text_span_vectors[test_indices]\n",
    "    y_train = categories[train_indices]\n",
    "    y_test = categories[test_indices]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll use a simple neural network consisting of:\n",
    "\n",
    "1. **Fully connected** layer with 256 nodes, relu activation\n",
    "2. **Dropout** 25% of the inputs\n",
    "3. **Fully connected** layer with 256 nodes, relu activation\n",
    "4. **Dropout** 25% of the inputs\n",
    "5. **Fully connected** layer with 11 nodes, softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Dense( {{choice([32, 64, 128, 256, 512, 1024])}}, batch_input_shape=(None, 100, )))\n",
    "    nn_model.add(Activation( {{choice(['relu', 'tanh', 'sigmoid'])}} ))\n",
    "    \n",
    "    if conditional( {{choice(['dropout', 'no dropout'])}} ) == 'dropout':\n",
    "        nn_model.add(Dropout( {{uniform(0, 1)}} ))\n",
    "    \n",
    "    nn_model.add(Dense(11))\n",
    "    nn_model.add(Activation('softmax'))\n",
    "    \n",
    "    nn_model.compile(loss='categorical_crossentropy', optimizer={{choice(['sgd', 'rmsprop', 'adagrad', 'adam', 'nadam'])}}, metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    #print(nn_model.summary())\n",
    "\n",
    "    tensorboard_callback = TensorBoard(log_dir='C:/tmp/pp_run-'+time.strftime(\"%Y-%m-%d-%H%M%S\"))\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1, mode='auto')\n",
    "    history = nn_model.fit(x_train, y_train, batch_size=128, epochs=64, verbose=2, validation_data=(x_test, y_test), callbacks=[early_stopping, tensorboard_callback])\n",
    "    acc = history.history['val_categorical_accuracy'][-1]\n",
    "    print('Test accuracy:', acc)\n",
    "    \n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': nn_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1b894aea208>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame('http://localhost:6006', '100%', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import imp\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import operator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import math\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import glob\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from IPython.display import IFrame\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import nltk\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nltk.tokenize import word_tokenize\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sn\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import hyperopt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Dense, Activation, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM, TimeDistributed\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping, TensorBoard\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import metrics\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils.np_utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import gensim\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [32, 64, 128, 256, 512, 1024]),\n",
      "        'Activation': hp.choice('Activation', ['relu', 'tanh', 'sigmoid']),\n",
      "        'conditional': hp.choice('conditional', ['dropout', 'no dropout']),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'optimizer': hp.choice('optimizer', ['sgd', 'rmsprop', 'adagrad', 'adam', 'nadam']),\n",
      "    }\n",
      "\n",
      ">>> Functions\n",
      "    1: def get_data():\n",
      "    2:     filenames, annotations = get_annotations()\n",
      "    3:     sanitized_policies = get_sanitized_policies(filenames)\n",
      "    4:     categories, categories_one_hot, category_lookup_table = get_categories()\n",
      "    5:     \n",
      "    6:     documents, categories = get_annotation_data(annotations, sanitized_policies, category_lookup_table)\n",
      "    7:     embedding_model = train_embeddings(documents)\n",
      "    8:     \n",
      "    9:     text_span_vectors = []\n",
      "   10:     for idx in range(len(categories)):\n",
      "   11:         text = ' '.join(documents[idx].words)\n",
      "   12:         vector = get_vector(embedding_model, text)\n",
      "   13:         text_span_vectors.append(vector)\n",
      "   14: \n",
      "   15:     return text_span_vectors, categories\n",
      "   16: \n",
      "   17: def get_vector(model, text):\n",
      "   18:     # set this so it's deterministic\n",
      "   19:     model.random = np.random.RandomState(1234)\n",
      "   20:     return model.infer_vector(word_tokenize(text))\n",
      "   21: \n",
      "   22: def train_embeddings(documents, epochs=16):\n",
      "   23:     model = gensim.models.Doc2Vec(vector_size=100)\n",
      "   24:     model.build_vocab(documents)\n",
      "   25:     model.train(documents, total_examples=len(documents), epochs=epochs)\n",
      "   26:     return model\n",
      "   27: \n",
      "   28: def get_annotation_data(annotations, sanitized_policies, category_lookup_table):\n",
      "   29:     chosen_categories = get_chosen_categories()\n",
      "   30:     attribute_value_types, attribute_value_values = get_attributes()\n",
      "   31:     remove_text = get_text_to_remove()\n",
      "   32:     \n",
      "   33:     #stemmer = nltk.stem.porter.PorterStemmer()\n",
      "   34:     lemmatizer = nltk.stem.WordNetLemmatizer()\n",
      "   35:     \n",
      "   36:     idx = 0\n",
      "   37:     \n",
      "   38:     documents = []\n",
      "   39:     categories = []\n",
      "   40:     \n",
      "   41:     remove_spans = {} # dictionary of policy ids and list of start, stop tuples that are then removed\n",
      "   42: \n",
      "   43:     '''\n",
      "   44:     remove_spans structure:\n",
      "   45:     \n",
      "   46:     {\n",
      "   47:     \"2\": --> this is the policy id\n",
      "   48:       {\n",
      "   49:        \"6\": [(20, 30), (30, 50)], --> this is the segment id\n",
      "   50:        \"8\": [(40, 123)] --> which maps to a list of tuple of start, end indices\n",
      "   51:       }\n",
      "   52:     }\n",
      "   53:     '''\n",
      "   54:         \n",
      "   55:     ##  first process the annotations\n",
      "   56:     for annotation_idx in range(len(annotations)):\n",
      "   57:         annotation = annotations[annotation_idx]\n",
      "   58:         \n",
      "   59:         for idx in range(len(annotation)):        \n",
      "   60:             category = annotation['Category Name'][idx]\n",
      "   61: \n",
      "   62:             if chosen_categories is None:\n",
      "   63:                 continue\n",
      "   64: \n",
      "   65:             if category not in chosen_categories:\n",
      "   66:                 continue\n",
      "   67: \n",
      "   68:             segment_id = annotation['Segment ID'][idx]\n",
      "   69:             \n",
      "   70:             if annotation_idx not in remove_spans:\n",
      "   71:                 remove_spans[annotation_idx] = {}\n",
      "   72:                 \n",
      "   73:             if segment_id not in remove_spans[annotation_idx]:\n",
      "   74:                 remove_spans[annotation_idx][segment_id] = []\n",
      "   75: \n",
      "   76:             # ok, we have our policy text, now we need to \n",
      "   77:             # remove all of the spans that are associated with a category\n",
      "   78:             # so we can attribute that text to the 'None' category\n",
      "   79: \n",
      "   80:             parsed = json.loads(annotation['Attributes/Values'][idx])\n",
      "   81:             for value in attribute_value_types:\n",
      "   82:                 if value not in parsed.keys():\n",
      "   83:                     continue\n",
      "   84: \n",
      "   85:                 attributes = parsed[value]\n",
      "   86:                 \n",
      "   87:                 if 'selectedText' not in attributes:\n",
      "   88:                     continue\n",
      "   89:                     \n",
      "   90:                 if 'startIndexInSegment' not in attributes:\n",
      "   91:                     continue\n",
      "   92:                     \n",
      "   93:                 if 'endIndexInSegment' not in attributes:\n",
      "   94:                     continue\n",
      "   95: \n",
      "   96:                 text = attributes['selectedText']\n",
      "   97:                 if text in remove_text:\n",
      "   98:                     continue\n",
      "   99:                     \n",
      "  100:                 start_idx = attributes['startIndexInSegment']\n",
      "  101:                 if start_idx == -1:\n",
      "  102:                     continue\n",
      "  103:                     \n",
      "  104:                 end_idx = attributes['endIndexInSegment']\n",
      "  105:                 if end_idx == -1:\n",
      "  106:                     continue\n",
      "  107: \n",
      "  108:                 remove_spans[annotation_idx][segment_id].append((start_idx, end_idx))\n",
      "  109: \n",
      "  110:                 text = text.lower()\n",
      "  111:                 processed_text = word_tokenize(text)\n",
      "  112:                 processed_text = [lemmatizer.lemmatize(word) for word in processed_text]\n",
      "  113: \n",
      "  114:                 doc = gensim.models.doc2vec.TaggedDocument(processed_text, [idx])\n",
      "  115:                 documents.append(doc)\n",
      "  116:                 \n",
      "  117:                 categories.append(category_lookup_table[category])\n",
      "  118:                 \n",
      "  119:                 idx += 1\n",
      "  120:     \n",
      "  121:     ## now process the remove spans from the policies\n",
      "  122:     if 'None' in chosen_categories:\n",
      "  123:         replace_items = [\"<br>\", \"<strong>\", \"</strong>\", \"<ul>\", \"</ul>\", \"<li>\", \"</li>\", \"<ol>\", \"</ol>\"]\n",
      "  124: \n",
      "  125:         for policy_idx in remove_spans:\n",
      "  126:             policy = sanitized_policies[policy_idx]\n",
      "  127: \n",
      "  128:             for segment_idx in remove_spans[policy_idx]:\n",
      "  129: \n",
      "  130:                 try:\n",
      "  131:                     policy_segment = policy[segment_idx]\n",
      "  132:                 except IndexError as e:\n",
      "  133:                     #print(e, policy_idx, segment_idx)\n",
      "  134:                     continue\n",
      "  135: \n",
      "  136:                 segment_text = policy_segment\n",
      "  137:                 for span in remove_spans[policy_idx][segment_idx]:\n",
      "  138:                     start_idx = span[0]\n",
      "  139:                     end_idx = span[1]\n",
      "  140:                     segment_text = segment_text[:start_idx] + \" \" + segment_text[end_idx:]\n",
      "  141: \n",
      "  142:                 segment_text = segment_text.lower()\n",
      "  143: \n",
      "  144:                 for item in replace_items:\n",
      "  145:                     segment_text = segment_text.replace(item, \" \")\n",
      "  146: \n",
      "  147:                 segment_text = segment_text.strip()\n",
      "  148: \n",
      "  149:                 if not segment_text: # check if we have any characters at all\n",
      "  150:                     continue\n",
      "  151: \n",
      "  152:                 processed_text = word_tokenize(segment_text)\n",
      "  153:                 processed_text = [lemmatizer.lemmatize(word) for word in processed_text]\n",
      "  154: \n",
      "  155:                 doc = gensim.models.doc2vec.TaggedDocument(processed_text, [idx])\n",
      "  156:                 documents.append(doc)\n",
      "  157: \n",
      "  158:                 categories.append(category_lookup_table['None'])\n",
      "  159: \n",
      "  160:                 idx += 1\n",
      "  161:     \n",
      "  162:     return documents, categories\n",
      "  163: \n",
      "  164: def get_text_to_remove():\n",
      "  165:     return ['null', 'Not selected']\n",
      "  166: \n",
      "  167: def get_chosen_categories():\n",
      "  168:     return ['First Party Collection/Use', \n",
      "  169:             'Third Party Sharing/Collection', \n",
      "  170:             'Other', \n",
      "  171:             'User Choice/Control', \n",
      "  172:             'Data Security',\n",
      "  173:             'International and Specific Audiences',\n",
      "  174:             'User Access, Edit and Deletion',\n",
      "  175:             'Policy Change',\n",
      "  176:             'Data Retention',\n",
      "  177:             'Do Not Track',\n",
      "  178:             'None' # added by us, not in original corpus\n",
      "  179:            ]\n",
      "  180: \n",
      "  181: def get_attributes():\n",
      "  182:     \n",
      "  183:     attribute_value_types = ['Access Scope',\n",
      "  184:      'Access Type',\n",
      "  185:      'Action First-Party',\n",
      "  186:      'Action Third Party',\n",
      "  187:      'Audience Type',\n",
      "  188:      'Change Type',\n",
      "  189:      'Choice Scope',\n",
      "  190:      'Choice Type',\n",
      "  191:      'Collection Mode',\n",
      "  192:      'Do Not Track policy',\n",
      "  193:      'Does/Does Not',\n",
      "  194:      'Identifiability',\n",
      "  195:      'Notification Type',\n",
      "  196:      'Other Type',\n",
      "  197:      'Personal Information Type',\n",
      "  198:      'Purpose',\n",
      "  199:      'Retention Period',\n",
      "  200:      'Retention Purpose',\n",
      "  201:      'Security Measure',\n",
      "  202:      'Third Party Entity',\n",
      "  203:      'User Choice',\n",
      "  204:      'User Type'\n",
      "  205:     ]\n",
      "  206: \n",
      "  207:     attribute_value_values = ['Additional service/feature',\n",
      "  208:      'Advertising',\n",
      "  209:      'Aggregated or anonymized',\n",
      "  210:      'Analytics/Research',\n",
      "  211:      'Basic service/feature',\n",
      "  212:      'Both',\n",
      "  213:      'Browser/device privacy controls',\n",
      "  214:      'Californians',\n",
      "  215:      'Children',\n",
      "  216:      'Citizens from other countries',\n",
      "  217:      'Collect from user on other websites',\n",
      "  218:      'Collect in mobile app',\n",
      "  219:      'Collect on first party website/app',\n",
      "  220:      'Collect on mobile website',\n",
      "  221:      'Collect on website',\n",
      "  222:      'Collection',\n",
      "  223:      'Computer information',\n",
      "  224:      'Contact',\n",
      "  225:      'Cookies and tracking elements',\n",
      "  226:      'Data access limitation',\n",
      "  227:      'Deactivate account',\n",
      "  228:      'Delete account (full)',\n",
      "  229:      'Delete account (partial)',\n",
      "  230:      'Demographic',\n",
      "  231:      'Does',\n",
      "  232:      'Does Not',\n",
      "  233:      'Dont use service/feature',\n",
      "  234:      'Edit information',\n",
      "  235:      'Europeans',\n",
      "  236:      'Explicit',\n",
      "  237:      'Export',\n",
      "  238:      'Financial',\n",
      "  239:      'First party collection',\n",
      "  240:      'First party use',\n",
      "  241:      'First-party privacy controls',\n",
      "  242:      'General notice in privacy policy',\n",
      "  243:      'General notice on website',\n",
      "  244:      'Generic',\n",
      "  245:      'Generic personal information',\n",
      "  246:      'Health',\n",
      "  247:      'Honored',\n",
      "  248:      'IP address and device IDs',\n",
      "  249:      'Identifiable',\n",
      "  250:      'Implicit',\n",
      "  251:      'In case of merger or acquisition',\n",
      "  252:      'Indefinitely',\n",
      "  253:      'Introductory/Generic',\n",
      "  254:      'Legal requirement',\n",
      "  255:      'Limited',\n",
      "  256:      'Location',\n",
      "  257:      'Marketing',\n",
      "  258:      'Mentioned, but unclear if honored',\n",
      "  259:      'Merger/Acquisition',\n",
      "  260:      'Named third party',\n",
      "  261:      'No notification',\n",
      "  262:      'Non-privacy relevant change',\n",
      "  263:      'None',\n",
      "  264:      'Not honored',\n",
      "  265:      'Not mentioned',\n",
      "  266:      'Opt-in',\n",
      "  267:      'Opt-out',\n",
      "  268:      'Opt-out link',\n",
      "  269:      'Opt-out via contacting company',\n",
      "  270:      'Other',\n",
      "  271:      'Other data about user',\n",
      "  272:      'Other part of company/affiliate',\n",
      "  273:      'Other users',\n",
      "  274:      'Perform service',\n",
      "  275:      'Personal identifier',\n",
      "  276:      'Personal notice',\n",
      "  277:      'Personalization/Customization',\n",
      "  278:      'Practice not covered',\n",
      "  279:      'Privacy contact information',\n",
      "  280:      'Privacy relevant change',\n",
      "  281:      'Privacy review/audit',\n",
      "  282:      'Privacy training',\n",
      "  283:      'Privacy/Security program',\n",
      "  284:      'Profile data',\n",
      "  285:      'Public',\n",
      "  286:      'Receive from other parts of company/affiliates',\n",
      "  287:      'Receive from other service/third-party (named)',\n",
      "  288:      'Receive from other service/third-party (unnamed)',\n",
      "  289:      'Receive/Shared with',\n",
      "  290:      'Secure data storage',\n",
      "  291:      'Secure data transfer',\n",
      "  292:      'Secure user authentication',\n",
      "  293:      'See',\n",
      "  294:      'Service Operation and Security',\n",
      "  295:      'Service operation and security',\n",
      "  296:      'Social media data',\n",
      "  297:      'Stated Period',\n",
      "  298:      'Survey data',\n",
      "  299:      'Third party sharing/collection',\n",
      "  300:      'Third party use',\n",
      "  301:      'Third-party privacy controls',\n",
      "  302:      'Track on first party website/app',\n",
      "  303:      'Track user on other websites',\n",
      "  304:      'Transactional data',\n",
      "  305:      'Unnamed third party',\n",
      "  306:      'Unspecified',\n",
      "  307:      'Use',\n",
      "  308:      'User Profile',\n",
      "  309:      'User account data',\n",
      "  310:      'User online activities',\n",
      "  311:      'User participation',\n",
      "  312:      'User profile',\n",
      "  313:      'User with account',\n",
      "  314:      'User without account',\n",
      "  315:      'View',\n",
      "  316:      'not-selected'\n",
      "  317:     ]\n",
      "  318:     \n",
      "  319:     return attribute_value_types, attribute_value_values\n",
      "  320: \n",
      "  321: def get_categories():\n",
      "  322:     categories = [\n",
      "  323:      'Data Retention', # 0\n",
      "  324:      'Data Security', # 1\n",
      "  325:      'Do Not Track', # 2\n",
      "  326:      'First Party Collection/Use', # 3\n",
      "  327:      'International and Specific Audiences', # 4\n",
      "  328:      'Other', # 5\n",
      "  329:      'Policy Change', # 6\n",
      "  330:      'Third Party Sharing/Collection', # 7\n",
      "  331:      'User Access, Edit and Deletion', # 8\n",
      "  332:      'User Choice/Control', # 9\n",
      "  333:      'None' # 10\n",
      "  334:     ]\n",
      "  335:     \n",
      "  336:     categories_one_hot = np.identity(len(categories))\n",
      "  337: \n",
      "  338:     category_lookup_table = {\n",
      "  339:      categories[0]:  categories_one_hot[0],\n",
      "  340:      categories[1]:  categories_one_hot[1],\n",
      "  341:      categories[2]:  categories_one_hot[2],\n",
      "  342:      categories[3]:  categories_one_hot[3],\n",
      "  343:      categories[4]:  categories_one_hot[4],\n",
      "  344:      categories[5]:  categories_one_hot[5],\n",
      "  345:      categories[6]:  categories_one_hot[6],\n",
      "  346:      categories[7]:  categories_one_hot[7],\n",
      "  347:      categories[8]:  categories_one_hot[8],\n",
      "  348:      categories[9]:  categories_one_hot[9],\n",
      "  349:      categories[10]: categories_one_hot[10],\n",
      "  350:     }\n",
      "  351:     \n",
      "  352:     return categories, categories_one_hot, category_lookup_table\n",
      "  353: \n",
      "  354: def get_sanitized_policies(annotation_files):\n",
      "  355:     sanitized_policies = []\n",
      "  356:     \n",
      "  357:     for file in annotation_files:\n",
      "  358:         with open(\"data\\\\sanitized_policies/{}.html\".format(file)) as f:\n",
      "  359:             sanitized_policies.append(f.readlines()[0].split('|||'))\n",
      "  360:             \n",
      "  361:     return sanitized_policies\n",
      "  362: \n",
      "  363: def get_annotations():\n",
      "  364:     filenames = []\n",
      "  365:     annotations = []\n",
      "  366:     \n",
      "  367:     header = ['Annotation ID', 'Batch ID', 'Annotator ID', 'Policy ID', 'Segment ID', 'Category Name', 'Attributes/Values', 'Policy URL', 'Date']\n",
      "  368:     keep_columns = ['Segment ID', 'Category Name', 'Attributes/Values']\n",
      "  369:     \n",
      "  370:     for file in glob.glob(\"data\\\\annotations/*.csv\"):\n",
      "  371:         filenames.append(file[17:-4])\n",
      "  372:         annotations.append(pd.read_csv(file, names=header)[keep_columns])\n",
      "  373:     \n",
      "  374:     return filenames, annotations\n",
      "  375: \n",
      "  376: \n",
      ">>> Data\n",
      "  1: \n",
      "  2: \n",
      "  3: text_span_vectors, categories = get_data()\n",
      "  4: \n",
      "  5: text_span_vectors = np.array(text_span_vectors)\n",
      "  6: categories = np.array(categories)\n",
      "  7: \n",
      "  8: choice = np.random.choice(len(text_span_vectors), len(text_span_vectors), replace=False)\n",
      "  9: test_percentage = 0.25 # keep 25% of data for testing\n",
      " 10: test_amount = math.floor(0.25 * len(text_span_vectors))\n",
      " 11: train_indices = np.array(choice[test_amount:])\n",
      " 12: test_indices = np.array(choice[:test_amount])\n",
      " 13: \n",
      " 14: # vecs, cats\n",
      " 15: x_train = text_span_vectors[train_indices]\n",
      " 16: x_test = text_span_vectors[test_indices]\n",
      " 17: y_train = categories[train_indices]\n",
      " 18: y_test = categories[test_indices]\n",
      " 19: \n",
      " 20: \n",
      " 21: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3: \n",
      "   4:     nn_model = Sequential()\n",
      "   5:     nn_model.add(Dense( space['Dense'], batch_input_shape=(None, 100, )))\n",
      "   6:     nn_model.add(Activation( space['Activation'] ))\n",
      "   7:     \n",
      "   8:     if conditional( space['conditional'] ) == 'dropout':\n",
      "   9:         nn_model.add(Dropout( space['Dropout'] ))\n",
      "  10:     \n",
      "  11:     nn_model.add(Dense(11))\n",
      "  12:     nn_model.add(Activation('softmax'))\n",
      "  13:     \n",
      "  14:     nn_model.compile(loss='categorical_crossentropy', optimizer=space['optimizer'], metrics=[metrics.categorical_accuracy])\n",
      "  15: \n",
      "  16:     #print(nn_model.summary())\n",
      "  17: \n",
      "  18:     tensorboard_callback = TensorBoard(log_dir='C:/tmp/pp_run-'+time.strftime(\"%Y-%m-%d-%H%M%S\"))\n",
      "  19:     early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=4, verbose=1, mode='auto')\n",
      "  20:     history = nn_model.fit(x_train, y_train, batch_size=128, epochs=64, verbose=2, validation_data=(x_test, y_test), callbacks=[early_stopping, tensorboard_callback])\n",
      "  21:     acc = history.history['val_categorical_accuracy'][-1]\n",
      "  22:     print('Test accuracy:', acc)\n",
      "  23:     \n",
      "  24:     return {'loss': -acc, 'status': STATUS_OK, 'model': nn_model}\n",
      "  25: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59753 samples, validate on 19917 samples\n",
      "Epoch 1/64\n",
      " - 2s - loss: 1.5378 - categorical_accuracy: 0.4481 - val_loss: 1.3693 - val_categorical_accuracy: 0.4690\n",
      "Epoch 2/64\n",
      " - 1s - loss: 1.3887 - categorical_accuracy: 0.4732 - val_loss: 1.3432 - val_categorical_accuracy: 0.4863\n",
      "Epoch 3/64\n",
      " - 2s - loss: 1.3644 - categorical_accuracy: 0.4831 - val_loss: 1.3310 - val_categorical_accuracy: 0.4916\n",
      "Epoch 4/64\n",
      " - 2s - loss: 1.3518 - categorical_accuracy: 0.4886 - val_loss: 1.3204 - val_categorical_accuracy: 0.4992\n",
      "Epoch 5/64\n",
      " - 2s - loss: 1.3395 - categorical_accuracy: 0.4956 - val_loss: 1.3124 - val_categorical_accuracy: 0.5024\n",
      "Epoch 00005: early stopping\n",
      "Test accuracy: 0.5024351056691555\n",
      "Train on 59753 samples, validate on 19917 samples\n",
      "Epoch 1/64\n",
      " - 2s - loss: 1.5440 - categorical_accuracy: 0.4486 - val_loss: 1.3627 - val_categorical_accuracy: 0.4702\n",
      "Epoch 2/64\n",
      " - 2s - loss: 1.4184 - categorical_accuracy: 0.4704 - val_loss: 1.3498 - val_categorical_accuracy: 0.4728\n",
      "Epoch 3/64\n",
      " - 2s - loss: 1.3939 - categorical_accuracy: 0.4749 - val_loss: 1.3376 - val_categorical_accuracy: 0.4742\n",
      "Epoch 4/64\n",
      " - 2s - loss: 1.3793 - categorical_accuracy: 0.4790 - val_loss: 1.3294 - val_categorical_accuracy: 0.4772\n",
      "Epoch 5/64\n",
      " - 2s - loss: 1.3660 - categorical_accuracy: 0.4824 - val_loss: 1.3239 - val_categorical_accuracy: 0.4772\n",
      "Epoch 00005: early stopping\n",
      "Test accuracy: 0.4771802981927966\n",
      "Train on 59753 samples, validate on 19917 samples\n",
      "Epoch 1/64\n",
      " - 4s - loss: 1.5754 - categorical_accuracy: 0.4222 - val_loss: 1.3777 - val_categorical_accuracy: 0.4665\n",
      "Epoch 2/64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-142c6b3853be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_annotation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_text_to_remove\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_chosen_categories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_attributes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_categories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_sanitized_policies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_annotations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbest_run\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhyperopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Privacy Policies and Neural Networks'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space)\u001b[0m\n\u001b[0;32m     65\u001b[0m                                      \u001b[0mfull_model_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                                      verbose=verbose)\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[1;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack)\u001b[0m\n\u001b[0;32m    131\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m              return_argmin=True),\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     )\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         )\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             return_argmin=return_argmin)\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     verbose=verbose)\n\u001b[0;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\School\\Research\\temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[1;34m(space)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1712\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\august.karlstedt\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "functions=[get_data, get_vector, train_embeddings, get_annotation_data, get_text_to_remove, get_chosen_categories, get_attributes, get_categories, get_sanitized_policies, get_annotations]\n",
    "best_run, best_model = optim.minimize(model=create_model, data=data, functions=functions, algo=hyperopt.rand.suggest, max_evals=4, trials=Trials(), notebook_name='Privacy Policies and Neural Networks')\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a confusion matrix to see the accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, just to get some stats on each category, the number of items in each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_lengths = {}\n",
    "\n",
    "for category in categories:\n",
    "    category_lengths[category] = len(df.loc[df['category'] == category])\n",
    "    print(category_lengths[category], 'examples in the', category, 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate our confusion matrix, we need the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix_columns = categories\n",
    "\n",
    "confusion_matrix = pd.DataFrame([], columns=confusion_matrix_columns)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_model.predict(X_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    row = pd.DataFrame([np.zeros(len(categories), dtype=np.int64)], columns=confusion_matrix_columns, index=[category])\n",
    "    \n",
    "    examples = df.loc[df['category'] == category]\n",
    "    indices = examples.index\n",
    "    \n",
    "    pred = predictions[indices]\n",
    "   \n",
    "    for i in range(len(indices)):\n",
    "        predicted_category = categories[np.argmax(np.round(pred[i]))]\n",
    "        row[predicted_category] += 1\n",
    "        \n",
    "    if category_lengths[category] > 0:\n",
    "        row /= category_lengths[category]\n",
    "        \n",
    "    confusion_matrix = confusion_matrix.append(row)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix.infer_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "sn.heatmap(confusion_matrix, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
